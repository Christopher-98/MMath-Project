---
title: "Project"
author: "Chris Martin"
date: "`r Sys.Date()`"
output: 
        bookdown::pdf_document2:
                fig_caption: yes

fontsize: 12pt
spacing: double
bibliography: references.bib
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

library(dsims)
library(knitr)
library(kableExtra)
library(dplyr)
library(moments)
```


# Abstract

# Introduction

The aim of this report is to detail to extensions made to the _dsims_ R package through the inclusion of density surface modelling (dsm) simulations. Analysis will be undertaken to compare the abundance estimates generated through a design based approach, distance sampling, and the model based approach, density surface modelling, to ensure the validity of the new method. By including the model based approach, this will allow researchers the opportunity to run simulations on distance sampling designs while allowing abundance estimations for specific regions to be generated through the density surface model. This ability to predict abundance estimates over very specific regions is a key reason for the inclusion of dsm, alongside the potential for more accurate error estimation than the existing methods used by distance sampling models. A variety of different simulations will be carried out to establish in which scenarios the 


# Background research

Informed from @Buckland2015
One of the key aims in areas of applied ecological research is to determine the abundance of a particular population of interest, such as in a periodic way to monitor its development over time and determine changes, or to evaluate the potential effect of a new factor, such as a human disturbance. The size of the population can determine the importance of any new factors, with a smaller populations more under threat from a given factor compared to an abundant one. One option for determining a populations size is to count every single individual, known as a census, similar to the UK completing a Census of its population every 10 years.
However, in the natural world, this is only realistically possible in the simplest instances and therefore a different approach must be used. Researches often use some form of sampling method to conduct a sample of the target population and draw conclusions for the overall population based on this sample.

The two most common methods of sampling for ecological populations are Mark-recapture and distance sampling, where information on the detectability of animals comes from the capture histories of individuals or the distances at which observations are made respectively. Mark and recapture methods, while important, are not the focus of this report and hence will not be further discussed. 

Distance sampling was first introduced by @Buckland1993 and includes a variety of techniques such  as line and point transect sampling, which can then be used to estimate animal abundance using information on the distances to the individuals or clusters observed. The underpinning theory is that if the probability of animal detection can be estimated based on the sample observed, this can allow for estimates on how many animals were not observed and can therefore correct the abundance estimates to take this information into account. 

The two simple techniques of distance sampling are, as mentioned above, line transect sampling and point transect sampling, with subsequent more complex techniques being extensions of these in one aspect or another. 
Line transect sampling consists of a set of lines being placed over a study area by some predetermined method, for example systematic with regular spacing between each or randomly generated. The observer moves along each line, known as a transect, looking for animals or animal groups, referred to as clusters. These are defined by @Buckland2015 as "a group of animals with a well defined location for the group centre." For any animal or cluster the observer detects at they make their way along each transect, the observer estimates or calculates the perpendicular distance _x_ of the animal or cluster from the nearest point of the line. 
For point transect sampling, the transects are a set of points placed over the study area, with different placement methods available as with line transects, however, most common is a systematically spaced grid. At each point, the observer records any individuals or clusters observed from the point, along with the distance _r_ from the point at which the observation was made. 

Distance sampling can therefore be thought of as a method of plot sampling, with the additional factor of not every animal on the plots being observed. For this, the plots in line transect sampling are rectangles of dimension $2wl$, where _l_ is the length of a given transect and may change between transects depending on the survey design and area shape, and _w_ is the truncation distance. This is the distance from the line, beyond which observations are not recorded if the truncation distance is determined prior to the study. However, if the truncation distance is determined during the analysis phase, then this is the distance beyond which the observations are excluded from the analysis. For the plots in point transect sampling, these are circles of area $\pi w^2$ with the plot radius _w_ being the truncation distance.

## Detection function

To take into account the fact that not all animals or clusters within each transect are observed, the probability of detecting an animal with a transect must be estimated. This is done by using the distances to the animals observed, _x_ for line transects and _r_ for point transects, to fit a detection function $g(x)$, defined as 'the probability of detecting an animal that is distance _x_ $(0\le x \le w)$ from the line'. This can be similarly defined as $g(r)$ for point transects where _r_$(0\le r \le w)$ is the distance from the point. The normal technique is to assume that all animals on the point or line are definitely observed, such that $g(0) = 1$. Several detection functions can be defined by the user and a model selection criteria can be used to select the most appropriate for the data. However, all good detection function models share a set of properties, namely, they should have a shoulder, be non-increasing, be model robust, have pooling robustness and be efficient. A shoulder is when the probability of detection remains close to 1 as distance from the transect increases, before decreasing at a later point. Non-increasing suggests that the probability of detection at a far distance should not exceed the probability at any shorter distance from the transect. Model robustness is necessary as the true function is never know and as such, any models must be flexible to allow a range of different profiles to be modelled. Pooling robustness is a property whereby it is assumed that the model will not be affected if any covariates which influence detection are not included in the model, however these can be included through the use of multi-covariate detection functions, part of Multiple-covariate distance sampling (MCDS). Efficiency informs that should all other factors be equal, a preferable model is on which gives high precision, although high precision should not outweigh the need for the other properties to be satisfied.  

## Distance Sampling Simulations

All the material in this section is based on @Buckland2015 Prior to the simulation for a particular design being run, a number of objects must be first be defined. The first object is the study region, this can either be the default generated by R or user defined from a shapefile. Following this, a spatial distribution or density surface must be defined, from which animal locations can be generated based on the population description. The desired population size can be user defined and set for a series of simulations or be generated based on the spatial distribution supplied by the user. The desired truncation distance must then be defined and based on this an appropriate design can be generated. The main considerations when constructing the design are the type, either line or point transects and the desired number or length of transects. If line transects are used, the design angle may be altered from its default of 0. A further parameter that must be set it whether plus or minus sampling should be used.  Based on the design, a set of survey transects can be generated, during which the detection process is simulated. Plus sampling is where the transects extend beyond the survey region into a buffer zone of distance _w_ from the edge, however only observations within the survey region are recorded.With minu sampling, transects end at the edge of the study region and do not extend beyond. Minus sampling will be used for all simulations throughout the remainder of this report. This is because in reality, most studies conduct minus sampling due to the additional cost associated with running a plus sampling survey, in addition to the fact that _dsims_ does not support plus sampling simulations at the present time. The user can then define a detection function, based on either a half normal ('hn'), hazard rate ('hr') or uniform distribution ('uf') with a defined scale parameter and the desired truncation distance _w_, examples of which can be seen in Figure \@ref(fig:detect) below:
```{r detect,  fig.show = 'hold', out.width='33%', fig.cap='Examples of different detection functions: Left: Half Normal, Middle: Hazard Rate, Right: Uniform'}
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/hn_detectfunc.jpg")))
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/hr_detectfunc.jpg")))
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/uf_detectfunc.jpg")))
```

Therefore for an animal at distance _x_ from the closest transect the probability of the animal being detected is given by the detection function evaluated at _x_, provided _x_ is less than or equal to _w_. The distance data generated during the survey is then analysed to estimate the abundance _N_ of the study area, with options available for several models to be analysed for each set of distance data, with a model selection criteria used to select the best, using AIC as the default. These operations are then repeated the specified number of times, say _R_, for each density and design, to obtain a set of simulations of animal distribution and survey design, alongside a corresponding set of estimates $\hat{N}$ of N. Typical values for _R_ are between 100 and 1000. In the case where the design is intrinsically selected by the user, as opposed to randomised, the exact same design will be used for all _R_ simulations

## Density Surface Modelling

This sections contains material based on @Miller2013.
In order to construct a density surface model, initial the approach must be decided upon. The choice is between using a two stage approach, whereby the detection function is fitted first then subsequently fitting a spatial model, while the one stage approach leads to estimating the detection and spatial parameters simultaneously. @Miller2013 states that 'Generally, very little information is lost by taking the two stage approach' as transect width is comparably smaller than that of the study region, therefore, provide the population does not differ spatially within the transect, no information is lost by the two stage approach. This may lead is issues occurring where the density of the species has significant variability at the transect level. However, one drawback of the two stage model is that, to accurately evaluate the model uncertainty, the uncertainty in both the detection function and the spatial models should be suitably combined. For the remainder of this report only the two stage approach will be discussed. Initially, the detection function must be fitted, with the specification being the same as mentioned in the distance sampling section above. Following this, the density surface model can be fitted. To enable this to occur, the data must be separated into segments. This is easily done for point transects with each point being a segment however it more complicated for line transects. With line transects, they must be split up into J segments of length $l_j$. It is normally from the segments to be approximately square, with dimensions of 2 _w_ x 2 _w_ where _w_ is the truncation distance of the design. From here, the segment areas enter the model as part of an offset, to allow for non-constant segment areas. This leads the line transect segments to have an area of 2$wl_j$ and the point transect segments with an area of $\pi w^2$. In the model, the counts or abundances are using a generalised additive model (GAM) using the sum of the smoothed covariates. 

### Response models

The model used when the count per segment is used as the response is:

$$ \mathbb{E}(n_j) = \hat{p_j}A_jexp[ \, \beta_0 + \sum_k f_k(z_{jk} ) ]\,$$

Where $f_k$ are the smoothed functions of the covariates and $\beta_0$ is the intercept term. By multiplying the segment area $A_j$ by the estimated probability of detection $p_j$ this gives the effective area of the segment, acting as an offset to account for different segment areas. Where distance is the only covariate in the detection function, $p_j$ is constant across all segments and therefore $\hat{p_j} = \hat{p} \forall j$. The distribution of $n_j$ can then be modeled using an overdispersed Poisson, Negative binomial or Tweedie distribution. 

An alternative to using this is to use abundance estimates for each segment generated by distance sampling as the response. To do this, the response $n_j$ is replaced by an estimator of the abundance in each section, $\hat{N_j}$ where this is defined as: 
$$ \hat{N_j} = \sum_{r = 1}^{R_j}  \frac{s_{jr}}{\hat{p_j}}$$

Where $R_j$ is the number of observations in the jth segment and $s_{jr}$ is the size of the rth group observed, with this being 1 if only individuals are observed. As identified by @Buckland2015, this is an Horvitzâ€“Thompson-like estimator of the segment abundance, allowing for covariates to be included through $\hat{p_j}$. The fitted model then becomes:
$$ \mathbb{E}(\hat{N_j}) = A_jexp [ \, \beta_0 + \sum_k f_k(z_{jk} ) ]\, $$
Where the model follows the same three distributions as before. The main difference between these models is that the offset is now the physical area of each segment, as opposed to the effective area in the first model for $n_j$.

To allow for a DSM to predict abundance, a series of prediction cells must be defined. These are not necessarily restricted to just the original study region, allowing for regions outside the study area to be predicted over. Each of the prediction cells must include the same covariates as specified in the dsm, including the area of each cell. Predictions can then be made for the abundance in each cell and by summing these over the whole region, an overall abundance estimate can be obtained. The size of the prediction cells may be specified by the user, however cells 'smaller than the resolution of the spatially referenced data' do not have an influence on the abundance estimates produced by dsm.

\pagebreak

# Simulation extentions

Within the _dsims_ package @R-dsims, the current simulation is set-up as explained within the the Distance Sampling Simulations section above. To allow for the inclusion of Density surface modelling within the simulation, a number of changes and additions were required to be made to ensure the compatibility of the two approachs within the simulation. This was a major focus of this project and was where a large portion of time was spent testing to ensure the generality of the code for future use. The initial setup required is the same as with the existing simulation. Namely, the study region, density surface, population and detection function are required. In addition to this, the design in question should also be specified, with any of the choices provided by _dsims_ available. 

Prior to the simulation beginning, the prediction grid required by the dsm is constructed across the study region, with the resolution of the grid set at half the truncation distance of the design. As @Miller2013 noted, smaller cells sizes could be used but there is a limit since using cells smaller than the spatial data resolution will not have an effect on the abundance estimates provided by dsm, and there is also the computational increase resulting from smaller cell sizes. This can be specified prior to the simulation as it is only dependant on the study region and will not change between iterations of the simulation. Existing code to construct this prediction grid can be found in @Souchay20 and this was used in early testing and simulations. However, it was noted that some of the R packages used in this code are due to be retired at the end of 2023, namely the _rgeos_ package @R-rgeos . Therefore, the decision was made to rewrite this code using the far more widely used Simple Feature (sf) package @R-sf. The prediction grid is currently setup to create the grid over the entire study region, to allow for a comparison with the distance sampling approach. However, this is not a restriction and any prediction grid could be specified, including areas outside of the study region. This would allow for simulations to predict the abundance on areas not specifically studied in the survey, which the distance sampling approach cannot do without increasing the size of the study region itself. This offers a distinct advantage for dsm over the distance sampling approach, however it may be possible to generate unrealistic abundance estimates if the gradient of the density surface is steep near the boundary. This effect will be tested later in this report. 

The simulation loop then begin, running for the number of iterations specified within the simulation object. For each iteration, a new survey constructed. From this, the observation data and segmented data is extracted and linked. To do this, a function _generate.dsm.data_ was written, taking the study region, survey and type of transects as inputs. This function first extracts the observation data from the survey. Subsequently, the transects are split into the segments required by the dsm approach, using a function _to_segments_ which was written for this explicit purpose, taking the same inputs as _generate.dsm.data_.

For Point transect designs, each point is treated as its own segment however, in the case of line transect designs, the lines are split to allow them to be modelled as points. Each line is split into segments of approximate length $2w$ with _w_ being the truncation distance, as suggested by @Miller2013 and each segment assigned its own unique sample label. Subsequently, the length of each segment is recorded as the segments effort value and the centre if the segment as its location. Polygons of each segment is then created using _st_buffer_, with _w_ being used as the distance. This leads to squares of approximately $2w * 2w$ for line transects and circles of radius _w_ for points. These allow the area of each segment to be calculated, a requirement for the dsm model. This is calculated using _st_area_, on the intersection between the polygons and the outer boundary of the study region. This ensures only areas within the study region are counted towards segment area and internal strata boundary's do not split segments. Failure to do this results in the areas of some segments being larger than they are in the survey, and as a result the dsm abundance estimate is smaller, since the prediction grid is only over the survey area.

Once the segment areas have been calculated, the segments can be linked to the observation data by allocating each observation to the nearest segment and giving this the respective segments sample label in the observation data, overwriting the original allocation. Additionally, the coordinate reference system of the observation data is set as that of the segments to ensure consistency when using different reference systems to the default. 

Based on this data, both a distance sampling model and density surface model are constructed, with the dsm modeling the counts against the smooth of spatial locations using a tweedie error distribution. The formulas for both could be changed to include environmental or other covariates such as strata, allowing for different detection functions to be fit to each strata. In the smoothed term, the degrees of freedom is restricted to the total number of transects. The limit for this is the number of segments in the model, which in the case of point transects is the total number of transects. While this could be far larger for line transects, this would increase the computational requirements beyond a reasonable level and not offer a significant improvement for this increase. The abundance estimates are extracted from both models and stored alongside other values from each including the standard errors and confidence intervals for each model.

\pagebreak

# Modelling - Default Region

The simulation was initially tested using the default region generated by _dsims_ @R-dsims with a truncation distance of 60. Both point and line transect designs were run with an aim of 25 and 12 samplers for the respective designs. A basic test density was then constructed for the region with high and low spots as seen below with relatively gently gradients. 

``` {r crop = TRUE}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default_density.jpg')))
```

A population description was then constructed based on this density surface with a true population of 1000. A detection function was then defined as a half normal with scale parameter of 30 for both designs, producing the following detection function:

```{r out.width='75%', fig.align='center'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default_detect.jpg')))
```

An example survey for each of the designs is displayed below in Figure \@ref(fig:defsurvey).
```{r defsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example Surveys for the default region. Top: Point Transect design, Middle: Parallel line design, Bottom: Zigzag line design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey zigzag.jpg')))
```


## Results

Having completed 1000 simulations for both the distance sampling and density surface models with each design, we can now examine and compare these to give us an insight into the circumstances under which a particular model is better of worse than the other.

### Default Region Point design

For the initial default region with the point transect design, the histograms of both the distance sampling and dsm abundance estimates are displayed below in Figure \@ref(fig:histdefpoint):
```{r histdefpoint, fig.show = 'hold', out.width='50%' , fig.cap='Histograms of Point Design estimates for Default region'}
default.point <- read.csv('Estimates/Default1000point.csv')

hist(default.point$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(default.point$dsm.est), col = 'red')

hist(default.point$ds.est,breaks = 50,
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(default.point$ds.est), col = 'red')
```

These plots show somewhat similar data since both estimates are generated by the same data set. If we now compare the means of the two approaches, the mean of DSM estimates was `r round(mean(default.point$dsm.est),2)` and the mean of DS estimates was `r round(mean(default.point$ds.est),2)`. It can be seen that the mean of the DSM estimates appears a good distance from the true population of 1000 while the distance sampling method is closer.

```{r resdefpoint}
results.point <- read.csv('Results/results default 1000 point.csv')
results.point$X <- NULL

results.point <- results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage')) %>%
        `colnames<-`(c('DSM results', 'DS results'))

kable(results.point,
      caption = 'Default region results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

Table \@ref(tab:resdefpoint) above provides statistics which allow us to compare the variability of the models as well as their ability to capture truth within their confidence intervals. We see in this case that ths DSM method has a slightly larger positve bias than that of the DS method, indicating the DS method produced more accurate results in this case. This trend continues with the Mean Standard errors and estimate standard deviations being similar but again the DS method producing a smaller error. The DSM does produce a smaller Coefficient of variation, however the difference is marginal. Next, we can evaluate how many times the true abundance was within the 95% CI for every model computed. The results show that truth was captured by the CI with a probability of above 0.9 for both models, with the DS again performing better at `r results.point[6,2]` compared with `r results.point[6,1]` for the DSM method.

### Default region parallel Line design

Now examining the results of the line transect design, on the same density surface, we see the histograms of the two estimates below in Figure \@ref(fig:histdefline):

```{r histdefline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Line Design estimates for Default region'}
default.line <-  read.csv('Estimates/Default1000line.csv')

hist(default.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(default.line$dsm.est), col = 'red')

hist(default.line$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(default.line$ds.est), col = 'red')
```

These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth. Investigating this further, we find the mean of DSM estimates to be `r round(mean(default.line$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(default.line$ds.est),2)`. These estimates are both very close to the true value, possibly as a result a large portion of the survey area being covered by the transects.
``` {r resdefline}
results.line <- read.csv('Results/results default 1000 line.csv')

results.line$X <- NULL

results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'Default region results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From table \@ref(tab:resdefline) above, it can be observed that in this case, the DSM method overall performed better than the DS method, having a slightly smaller bias as well as a dramatically smaller mean standard error and mean coefficient of variation. However, for the DS method, the standard deviation of the estimates is markedly less than the mean standard error, indicating this method provides better precision than the standard errors suggest. This is also the case with the DSM method however the difference is far smaller. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`.

### Default region Zigzag Line design

Now examining the results of the zigzag line transect design, on the same density surface, we see the histograms of the two estimates below in Figure \@ref(fig:histdefzig) :

```{r histdefzig, fig.show = 'hold', out.width='50%' , fig.cap='Histograms of Zigzag Design estimates for Default region'}
default.zig <-  read.csv('Estimates/Default1000linezigzag.csv')

hist(default.zig$dsm.est,breaks = 50, main = 'Histogram of DSM estimates')
abline(v = mean(default.zig$dsm.est), col = 'red')
hist(default.zig$ds.est,breaks = 50, main = 'Histogram of DS estimates')
abline(v = mean(default.zig$ds.est), col = 'red')
```

These plots differ slightly to their point and parallel line counterparts, with the mean appearing further away from truth than previously, however there is a known error in the code that may cause this and is being investigated. Investigating this further, we find the means of DSM estimates to be `r round(mean(default.zig$dsm.est),2)` and the mean of DS estimates to be `r round(mean(default.zig$ds.est), 2)`. These estimates are both very close to the true value, possibly as a result a large portion of the survey area being covered by the transects.

```{r resdefzig}
results.zig <- read.csv('Results/results default 1000 zigzag.csv')
results.zig$X <- NULL

results.zig <- results.zig %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zig,
      caption = 'Default region results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Zigzag Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "HOLD_position")
```

From Table \@ref(tab:resdefzig), despite the known issue, the DSM model performs relatively well, having a reasonably small bias. The mean standard error is in fact lower than that of the DS model, indicating higher precision on average across the models, with the standard deviation of the estimates being lower in both cases, suggesting greater precision for both modes than the standard errors imply. The mean coefficient of variation is slightly lower for the DSM model however the confidence interval coverage is higher with the DS model, indicating truth has a higher probability of being captured within the models confidence interval. 

\pagebreak

# Modeling - North Sea

Having gone through the process and results for the default region produced by _dsims_, a more complex example was created to further test the capabilities of the simulations with a real world example. This region with be in the area of the North sea off the east coast of the UK. As this would likely be a shipborne or airborne study, point transects are not realistic in this scenario, however they will be included for completeness. To assess the performance of both the distance sampling and density surface modelling approaches, point and line transect designs will be examined. These will be two point transect designs and four line transect designs, with one of each type using a stratified design with the region split in two. The standard designs will be compared first before examining the stratified designs. All designs used a truncation distance of 10 with the number of transects varying between the point and line transect designs. This difference is ensure the covered area of each study is similar to allow the results to be compared, as well as ensure sufficient observations to fit the detection function can be obtained. The density maps for stratified and non-stratified will be different to justify the use of different strata and examine the models ability to pick this up. These density surfaces can be seen in Figure \@ref(fig:NSden) below.
```{r NSden, crop = TRUE, fig.show='hold', out.width='50%', fig.cap='Density surfaces for the basic and stratified North Sea regions'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North_Sea_density.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North_Sea_Strata_density.jpg')))

```
As before, the detection function was kept identical between each design for both stratified and non-stratified. In this case it was a Half Normal detection function with a scale parameter of 5, which can be seen in Figure \@ref(fig:NSdet) below:
```{r NSdet, out.width='75%', fig.align='center', , fig.cap='Plot of the detection function used in both the basic and stratified North Sea regions: A half Normal with scale parameter 5 and truncation 10'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Detect.jpg')))
```
## Non-Stratified Designs

The point transect designs aimed for 70 samplers and the line transects for 25 samplers. The standard line designs consisted of a systematic parallel line design with transects perpendicular to the coast and a zigzag line design originating in the North of the region. An example of each design can be seen in Figure \@ref(fig:NSsurvey) below:
```{r NSsurvey, fig.show = 'hold', out.width='45%', out.height='75%', fig.cap='Examples of surveys for the North Sea region. Top Left: Point Transect design, Top Right: Parallel line design, Bottom Left: Zigzag line design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey zigzag.jpg')))
```

The simulation was run 5000 times with the dsm model using a tweedie error distribution and the only covariates being spatial coordinates.

\pagebreak

### Non Stratifed Results - Point Design

For the point transect design, the histograms of the abundance estimates from each simulation run are displayed below in Figure \@ref(fig:histNSpoint):
```{r histNSpoint, fig.show = 'hold', out.width='50%', fig.cap='Histograms of the abundance estimates for the North Sea region using a point transect design'}
NS.point <- read.csv('Estimates/North Sea5000point.csv')

hist(NS.point$dsm.est,
     breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.point$dsm.est), col = 'red')

hist(NS.point$ds.est,
     breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.point$ds.est), col = 'red')
```

These plots suggest both methods are relatively good at estimating the true abundance of 1000, with both methods appearing to overestimate, with the DSM method overestimating slightly more than the DS method. There also appears to be a slight right skewness to both plots which will be looked at analytically. Investigating the means further, we find the mean of DSM estimates to be `r round(mean(NS.point$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(NS.point$ds.est),2)`. These estimates are both close to the true abundance, with the overestimation present as mentioned above. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NS.point$dsm.est - NS.point$ds.est) > 0) / nrow(NS.point) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.point$dsm.est, NS.point$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. If we now compare the other variables extracted from the models in the below, we can examine the accuracy and variability of each method:
``` {r }
results.point <- read.csv('Results/results NS 5000 point.csv')

results.point$X <- NULL

results.point <-  results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.point,
      caption = 'North Sea Non-Stratified results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From the results table above, it can be observed that in this case, the DS method overall performed better than the DSM method, having a smaller bias as well as higher confidence interval converge, despite having a smaller Mean Std error. Additionally, the standard deviation of the estimates is slightly less for the DS method, however both methods do a relatively good job at estimating this. Additionally, we observe from the skewness statistics that both methods returned a positive statistic indicating the right-skewed as identified above, which suggests the right tail is longer and this is supported by some of the estimates being over 2000, with the maximums for DSM and DS being `r round(max(NS.point$dsm.est),2) ` and `r round(max(NS.point$ds.est),2)` respectively. The kurtosis figures for both methods are greater than 3, indicating we would expect more frequent outliers than a normal distribution   While the Mean CV for the DSM method is lower, the difference is marginal and does not out-way the bias or variance issues of this method. Overall this clearly seems to be a case where the DS method performs better than the DSm estimation method.

### Non Stratifed Results - Parallel Line Design

Now examining the results of the line transect design, on the same density surface, we see the histograms of the two estimates below:

```{r fig.show = 'hold', out.width='50%'}
NS.line <-  read.csv('Estimates/North Sea5000line.csv')

hist(NS.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.line$dsm.est), col = 'red')

hist(NS.line$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.line$ds.est), col = 'red')
```

These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth.However, there appears to be less skewness for these than is present with the point transect results. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NS.line$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NS.line$ds.est), 2)`. These estimates are very close to the true Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NS.line$dsm.est - NS.line$ds.est) > 0) / nrow(NS.line) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.line$dsm.est, NS.line$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.
``` {r }
results.line <- read.csv('Results/results NS 5000 line.csv')

results.line$X <- NULL

results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'North Sea Non-Stratified results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From the results table above, it can be seen that in this case, the performance of both models was similar and each performed well in different areas compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better error estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`, however this difference is likely due to the higher Std errors from the DS method wideing the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the kurtosis is very similar to that of a normal distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NS.line$dsm.est)$p.value` and `r jarque.test(NS.line$ds.est)$p.value` respectively. These indicate we can not reject the null hypothesis that these estimates are normally distributed. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 

### Non Stratifed Results - Zigzag Line Design

Now examining the results of the zigzag line design, on the same density surface, we see the histograms of the two estimates below:

```{r fig.show = 'hold', out.width='50%'}
NS.zig <-  read.csv('Estimates/North Sea5000linezigzag.csv')

hist(NS.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.zig$dsm.est), col = 'red')

hist(NS.zig$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.zig$ds.est), col = 'red')
```

These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth.However, there appears to be less skewness for these than is present with the point transect results. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NS.zig$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NS.zig$ds.est), 2)`. These estimates are very close to the true Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NS.zig$dsm.est - NS.zig$ds.est) > 0) / nrow(NS.zig) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.zig$dsm.est, NS.zig$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.
``` {r }
results.zigzag <- read.csv('Results/results NS 5000 zigzag.csv')

results.zigzag$X <- NULL

results.zigzag <-  results.zigzag %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))

kable(results.zigzag,
      caption = 'North Sea Non-Stratified results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From the results table above, it can be seen that in this case, the performance of both models was similar and each performed well in different areas compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better error estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.zigzag[6,2]` compared to `r results.zigzag[6,1]`, however this difference is likely due to the higher Std errors from the DS method wideing the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the kurtosis is very similar to that of a normal distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NS.zig$dsm.est)$p.value` and `r jarque.test(NS.zig$ds.est)$p.value` respectively. These indicate we can not reject the null hypothesis that these estimates are normally distributed. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 

## Stratified Designs

For the stratified designs, it was decided to split the region into two strata, a large northern strata 'North' and a smaller southern strata 'South' to test how the simulation and both methods cope with a stratified design. The point transect designs aimed for 60 samplers, with 30 in each strata, and the line transects for 25 samplers with 10 in the North strata and 15 in the South strata.. The standard line designs consisted of a systematic parallel line design with transects perpendicular to the coast for both the North and South Stratum. The zigzag line design had the transects in the North strata originating in the North of the Strata, with a design angle of 90', while in the South Strata, transects originate on the Western boundary with a design angle of 0'. An example of each design, including the point design, can be seen in Figure \@ref(fig:NSSsurvey) below:
```{r NSSsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example surveys for each Stratified Design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey zigzag.jpg')))
```

The simulation was run 5000 times with the dsm model using a tweedie error distribution and the only covariates being spatial coordinates.

### Stratifed Results - Point Design

For the point transect design, the histograms of the abundance estimates from each simulation run are displayed below in Figure \@ref(fig:histNSSpoint):
```{r histNSSpoint, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Point Design estimates for Straatified North Sea Region'}
NSS.point <- read.csv('Estimates/North Sea Strat5000point.csv')

hist(NSS.point$dsm.est,
     breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.point$dsm.est), col = 'red')

hist(NSS.point$ds.est,
     breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.point$ds.est), col = 'red')
```

These plots suggest both methods are relatively good at estimating the true abundance of 1000, with both methods appearing to overestimate, with the DSM method overestimating slightly more than the DS method. There again appears to be a slight right skewness to both plots which will be looked at analytically below. Investigating the means further, we find the mean of DSM estimates to be `r round(mean(NSS.point$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(NSS.point$ds.est),2)`. These estimates are both close to the true abundance, with the overestimation present as mentioned above and to a very similar level to the Non-Stratified point results. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.point$dsm.est - NSS.point$ds.est) > 0) / nrow(NSS.point) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NSS.point$dsm.est, NSS.point$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the difference between the DSM and DS estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. If we now compare the other variables extracted from the models below, we can examine the accuracy and variability of each method:
``` {r resNSSpoint}
results.point <- read.csv('Results/results NS Strat 5000 point.csv')

results.point$X <- NULL

results.point <-  results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.point,
      caption = 'North Sea Stratified results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From Table \@ref(tab:resNSSpoint) above, it can be observed that in this case, the DS method overall performed better than the DSM method, having a smaller bias as well as higher confidence interval coverage, despite having a smaller Mean Std error. Additionally, the standard deviation of the estimates is slightly less for the DS method, however both methods do a relatively good job at estimating this. Additionally, we observe from the skewness statistics that both methods returned a positive statistic indicating the right-skewed as identified above, which suggests the right tail is longer and this is supported by some of the estimates being close to 2000, with the maximums for DSM and DS being `r round(max(NSS.point$dsm.est),2) ` and `r round(max(NSS.point$ds.est),2)` respectively. The kurtosis figures for both methods are greater than 3, indicating we would expect more frequent outliers than a normal distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NSS.point$dsm.est)$p.value` and `r jarque.test(NSS.point$ds.est)$p.value` respectively. These indicate we can reject the null hypothesis that these estimates are normally distributed, therefore our use of the Wilcoxon test above is justified.    While the Mean CV for the DSM method is lower, the difference is marginal and does not out-way the bias or variance issues of this method. Overall this clearly seems to be a case where the DS method performs better than the DSM estimation method.

### Stratifed Results - Parallel Line Design

Now examining the results of the line transect design, on the same density surface, we see the histograms of the estimates below in Figure \@ref(fig:histNSSline):

```{r histNSSline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Parallel Design estimates for North Sea Region' }
NSS.line <-  read.csv('Estimates/North Sea Strat5000line.csv')

hist(NSS.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.line$dsm.est), col = 'red')

hist(NSS.line$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.line$ds.est), col = 'red')
```

These plots are similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth. However, there appears to be less skewness for these than is present with the point transect results, exhibiting a slightly sharper point at the mean. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NSS.line$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NSS.line$ds.est), 2)`. These estimates are very close to the truth, with DSM again overestimating in comparison to DS. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.line$dsm.est - NSS.line$ds.est) > 0) / nrow(NSS.line) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS. `r test <- wilcox.test(NSS.line$dsm.est, NSS.line$ds.est, paired = T, alternative = 'greater')` This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the difference between the DSM and DS estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.
``` {r resNSSline}
results.line <- read.csv('Results/results NS Strat 5000 line.csv')

results.line$X <- NULL

results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'North Sea Stratified results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From Table \@ref(tab:resNSSline) above, it can be seen that in this case, the performance of both models was very similar throuhgout all areas, with the exception being the mean estimates and bias. The mean estimate for DSM was again a slight overestimation and as a result, bias was positive, while DS underestimated by almost the same amount, leading to an almost identical but negative bias. The Mean Std errors and Std Deviation of estimates were slightly lower for DSM, however the difference in both cases is very small. The Mean CV for each method is very similar, being slightly in favour of the DSM method although the difference is negligible. The DS method has a slightly higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`, however this small difference is likely due to the slightly higher Std errors from the DS method widening the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is small. The kurtosis scores are close to 3 but larger, suggesting the distribution of the estimates are is similar to that of a normal distribution, with a slight trend towards a leptokurtic distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NSS.line$dsm.est)$p.value` and `r jarque.test(NSS.line$ds.est)$p.value` respectively. These indicate we can reject the null hypothesis that these estimates are normally distributed and our use of the Wilcoxon test above is justified, since the normality assumption would have been violated for a parametric test. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 

### Stratifed Results - Zigzag Line Design

Unfortunately, for the stratified zigzag design, an error intermittently occurred within the simulation which caused the fitting of the density surface model to fail. Upon further inspection, this error did not occur within any of the code written as part of this project. After consultation with L Marshall, the author of the _dsims_ package, it was determined the error likely occurred as a result of how some of the survey realisations within _dsims_ were constructed. This likely led to issues with the coordinates for the transects and segments, with this not being an issue within the DS model however the density surface model would not accept these, resulting in the error. It is outside the scope of this project to investigate the root cause of this error, therefore L Marshall was notified of the bug and given access to the code to determine the root cause. As mentioned, this error occurred intermittently, resulting in a full set of 5000 iterations not being run. However, the largest number run before the error occurred was xxx and these results are included below for completeness. Examining these results, we see the histograms of the two estimates below:

```{r fig.show = 'hold', out.width='50%'}
NSS.zig <-  read.csv('Estimates/North Sea5000linezigzag.csv')

hist(NSS.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.zig$dsm.est), col = 'red')

hist(NSS.zig$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.zig$ds.est), col = 'red')
```

These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth.However, there appears to be less skewness for these than is present with the point transect results. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NSS.zig$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NSS.zig$ds.est), 2)`. These estimates are very close to the true Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.zig$dsm.est - NSS.zig$ds.est) > 0) / nrow(NSS.zig) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.zig$dsm.est, NS.zig$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.
``` {r }
results.zigzag <- read.csv('Results/results NS 5000 zigzag.csv')

results.zigzag$X <- NULL

results.zigzag <-  results.zigzag %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zigzag,
      caption = 'North Sea Non-Stratified results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From the results table above, it can be seen that in this case, the performance of both models was similar and each performed well in different areas compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better error estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.zigzag[6,2]` compared to `r results.zigzag[6,1]`, however this difference is likely due to the higher Std errors from the DS method wideing the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the kurtosis is very similar to that of a normal distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NSS.zig$dsm.est)$p.value` and `r jarque.test(NSS.zig$ds.est)$p.value` respectively. These indicate we can not reject the null hypothesis that these estimates are normally distributed. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 

\pagebreak

# Modeling - Buckland 2015 Example

For our next set of simulations to examine the capabilities of the DSM approach in respect to the DS method, I will revisit the the example simulation provided in Section 2.5.2 of @Buckland2015 and use both a DS and DSM approach to repeat these simulations. It must be noted that the original analysis was part of a survey design case study by Laura Marshall and used the R package _DSsim_ @R-DSsim . For the purpose of this study, each of the designs and simulations will be translated into the _dsims_ @R-dsims package used throughout the remainder of the simulations to ensure comparability with the DSM approach developed. All details from the original designs will be kept constant, with the population being fixed at 1500 individuals and using a truncation distance of 1000m. For each of the surveys, a half-normal detection function with a scale parameter of 500 will be assumed. The density surfaced used is displayed below alongside the detection function in Figure \@ref(fig:Montdensdetect).
```{r Montdensdetect, fig.show = 'hold', out.width='50%', fig.cap='Density surface and detection function for Buckland 2015 example'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave density.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave detect.jpg')))
```


By using this study region, it is possible to verify the simulation method developed for DSM can be run on study regions with internal holes, such as islands on a seaborne survey. The three different designs will be used, being the same as those tested in the case study in @Buckland2015. These will be a systematic parallel line design, a random parallel line design and a zigzag design. Examples of each design are displayed in Figure \@ref(fig:Montsurvey) below:

```{r Montsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example surveys for each Design: Top Left: Random Parallel Line, Top Right: Systematic Parallell line, Bottom Left: Zigzag line'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey random line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey parallel line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey zigzag.jpg')))
```
As with each of the simulations run as part of the North Sea section, 5000 iterations will be run for each design. While this is different from the 100 iterations originally conducted, the increase in the number of iterations will increase the reliability of the results.

## Results - Random Parallel Line

We can now examine the results of the Random parallel line design by initially plotting histograms of the estimates from both the DS and DSM methods below in Figure \@ref(fig:histMontrand):

```{r histMontrand, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Random Parallel Design estimates for Buckland 2015 example region'}
Mont.rand <-  read.csv('Estimates/Montrave5000linerandom.csv')

hist(Mont.rand$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(Mont.rand$dsm.est), col = 'red')

hist(Mont.rand$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(Mont.rand$ds.est), col = 'red')
```

While the plot for the DS estimates appears similar to those we have seen before, the estimates for the DSM method appear markedly different. This is due to the prescence of very extreme overestimates with the largest estimate being `r round(max(Mont.rand$dsm.est))`, over 3 times the true population. It initially appears that the majority of the DSM estimates are centred around the true population of 1500, however this will be investigated further below. As with the previous simulations, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((Mont.rand$dsm.est - Mont.rand$ds.est) > 0) / nrow(Mont.rand) *100` % of the time, indicating it is likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(Mont.rand$dsm.est, Mont.rand$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS.

``` {r resMontrand}
results.rand <- read.csv('Results/results Montrave random.csv')

results.rand$X <- NULL

results.rand <-  results.rand %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.rand,
      caption = 'Buckland 2015 example results - Random Line') %>%
        add_header_above(c('Statistic' = 1, 'Random Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "HOLD_position")
```
In Table \@ref(tab:resMontrand) above, we can see the various statistics extracted during the course of the simulations. 


## Results - Systematic Parallel Line

``` {r}
results.para <- read.csv('Results/results Montrave parallel.csv')

results.para$X <- NULL

results.para <-  results.para %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.para,
      caption = 'Buckland 2015 example results - Parallel Line') %>%
        add_header_above(c('Statistic' = 1, 'Systematic Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

## Results - Zigzag Line

``` {r}
results.zig <- read.csv('Results/results Montrave zigzag.csv')

results.zig$X <- NULL

results.zig <-  results.zig %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zig,
      caption = 'Buckland 2015 example results - Zigzag Line') %>%
        add_header_above(c('Statistic' = 1, 'Zigzag Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

\pagebreak

# Modeling - Variance estimation tests

\pagebreak

# Modeling - North Sea Predictions outside study area:

As mentioned within the simulation etentions section, the prediction grid for DSM is not limited to the area of the study region, it can be smaller or larger and does not even need to include any part of the study region. However, predicting populations outside of the original study region may be subject to large variations dependant on the gradient of the density surface at the edge of the region. This will be tested by using the same North Sea region we have explored before, however with two very contrasting density surfaces to show the two extremes of what could happen in this scenario. As this simulation will focus on predicting the abundance outside of the study region, the estimates from the DS method will not be compared here as the predictions will be made over a far larger area. As a simple example, the prediction grid used will be based on the bounding box of the North Sea region, in essence a large rectangle encompassing the entirety of the region.

The two contrasting density surfaces can be seen in Figure \@ref(fig:densityOut)
```{r densityOut, fig.show = 'hold', out.width='50%', fig.cap='Density surface and detection function for Montrave region'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/NSout Low edge den.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/NSout High edge den.jpg')))
```

In a slight difference to the original simulations, the number of transects was reduced to 20 to reduce the amount of area covered by each study. All other aspects of the original simulation were kept the same, with the detection function  being the same half normal function with a scale parameter of 5 with a truncation distance of 10 seen in Figure \@ref(fig:NSdet).

## Results

```{r histNSout, fig.show = 'hold', out.width='50%', fig.cap='Histograms of DSM abundance estimates for the North Sea region when predicting outside the study region using different density surfaces'}
NSout.high <-  read.csv('Estimates/North Sea Extreme5000outside.csv')
NSout.low <-  read.csv('Estimates/North Sea Break5000outside.csv')

hist(NSout.low$dsm.est,breaks = 50,
     main = 'Histogram of estimates for Low edge density',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSout.low$dsm.est), col = 'red')

hist(NSout.high$dsm.est,breaks = 50, 
     main = 'Histogram of estimates for High edge density',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSout.high$dsm.est), col = 'red')

```


```{r resNSout}
results.low <- read.csv('Results/results NSout Low edge den.csv')
results.high <- read.csv('Results/results NSout High edge den.csv')

results.dsm.low <- results.low$dsm.results
results.dsm.high <- results.high$dsm.results

results.outside <- data.frame(low = results.dsm.low,
                              high = results.dsm.high)
# remove bias and confidence interval coverage as not accurate

results.outside <- results.outside[-c(2, 6), ]


results.outside <-  results.outside %>%
        `rownames<-`(c('Mean',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('Low Edge Density', 'High Edge Density'))
kable(results.outside,
      caption = 'North Sea region with different densities when predicting outside the study region') %>%
        add_header_above(c('Statistic' = 1, 'DSM results' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

\pagebreak
# Discussion

\pagebreak
# References {-}
