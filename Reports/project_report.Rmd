---
title: "Wildlife Abundance estimation: Distance Sampling and Density Surface Modeling Simulations"
author: "Christopher Martin"
date: "`r Sys.Date()`"
output: 
        bookdown::pdf_document2:
                fig_caption: yes
                toc: false

fontsize: 12pt
spacing: double
bibliography: references.bib
link-citations: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

library(dsims)
library(knitr)
library(kableExtra)
library(dplyr)
library(moments)
```

```{r out.height = '50%', fig.align='center', out.width= '50%'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/university crest.jpg')))
```

Decleration:
_I certify that this project report has been written by me, is a record of work carried out by me, and is essentially different from work undertaken for any other purpose or assessment._

All of the code discussed and used throughout this report can be found at the following GitHub repository: https://github.com/Christopher-98/MMath-Project

\newpage
\tableofcontents




\pagebreak

# Abstract

Abundance estimation for wildlife populations is a key area of applied ecological research, with one of the most common techniques used for being distance sampling. This assumes not all animals are detected within the study area and uses detection distances to fit a function accounting for those missed. Density surface modelling (DSM) extends this idea by using these detection distance and creating a spatial model, from which abundance predictions can be generated across the study area. This report has focused on incorporating DSM within the existing distance sampling simulations, to allow designs to be examined by both distance sampling analysis and density surface modelling.

A variety of simulations were constructed across different regions, with each simulation aiming to examine a particular area of interest. The results of these produced intriguing conclusions. DSM appears to have a positive bias, overestimating the true abundance in every simulation. For Point designs, DSM gives little improvement over the existing distance sampling approach. However, for line transects, variance estimation using the default estimators indicated that distance sampling overestimates the variance. This lead to DSM having more precise abundance estimates, as noted by @Miller2013. In efforts to improve this, using more appropriate variance estimators was identified as a suitable adjustment, provided the design permitted this. An alternative to this was discovered when the region was stratified, with the variance estimates in these cases being comparable between distance sampling and DSM. 

\pagebreak 

# Introduction

One of the key aims in areas of applied ecological research is to determine the abundance of a particular population of interest, such as in a periodic way to monitor its development over time and determine changes, or to evaluate the potential effect of a new factor, such as a human disturbance. The size of the population can determine the importance of any new factors, with a smaller populations more under threat from a given factor compared to an abundant one @Buckland2015.

The aim of this report is to examine and investigate through simulations the extensions made to the _dsims_ R package through the inclusion of density surface modelling (DSM) simulations. The intended readership of this report are biologists or ecologists with statistical experience in distance sampling for estimating wildlife abundance. Further knowledge on distance sampling survey design would be beneficial to understanding some elements of the report in greater detail however this is not required.

Distance Sampling (@Buckland2001) is an extensive set of techniques used for estimating abundance within biological populations. These techniques have been widely used in both terrestrial ecology and aquatic studies to determine population size or density. The key idea of this approach is the assumption that not all animals are observed. Therefore, the probability of an animal being detected is estimated using the recorded distances at which observations are made. This allows for the total observations to be corrected to account for those animals missed and achieve an overall abundance estimate (@Buckland2015).

Density surface modelling builds upon the techniques of distance sampling by relating animal density to spatial variables within the environment, such as topology and habitat, while still correcting for animals not observed. The goal of this is to provide more information to wildlife managers to allow them to more appropriately manage the environment and the animals within @Hedley2004. This has lead to an increase in demand for spatial models, especially for analysing line transect data. These models, in addition to providing abundance estimates, allow for relationships between covariates and abundance to be investigated (@Miller2013).

The _dsims_ R package (@R-dsims) was created to allow researchers to randomise the construction of distance sampling designs to examine both the efficiency and bias of different designs with the goal of selecting the optimum survey design. This provides a great benefit to researchers, as failing to randomise the design selection can result in abundance estimates having considerable bias (@Buckland2015).

Currently, _dsims_ does not fit a density surface model during the course of its simulations, despite more precise abundance estimates, as identified by @Kat2007, alongside smaller variance estimates noted by @Miller2013 being available. Therefore, by including density surface modelling, improvements can be made to the estimates provided during these simulations. 

The more precise abundance estimates provided by density surface modelling, identified by @Kat2007, alongside smaller variance estimates noted by @Miller2013, are the key reasons for the inclusion of density surface modelling within simulation set-up. The inclusion therefore provides an added level of assurance to design selection decisions, as distance sampling simulations are used to determine the optimum survey design to estimate animal abundance for real world studies. For example, if a researcher wishes to undertake both density surface modelling and distance sampling analysis, the inclusion of both within the simulation at the design stage provides assurances that the optimum design for distance sampling analysis is also the optimum design for density surface modelling. Should there be no design which is optimum for both methods, the researcher can select which analysis type they wish prioritise and determine the optimum design for this.

Simulations will be designed and implemented to compare the accuracy and precision of abundance estimates generated through the design based approach, distance sampling, and the model based approach, density surface modelling. This is to ensure that the density surface modelling approach has successfully been implemented into the simulation set-up alongside the existing distance sampling approach.

\pagebreak

# Background research

## Plot Sampling

In order to determine a populations size, a census is one option available to those conducting the study. This involves counting every single individual within the population, similar to the UK completing a Census of its population every 10 years. However, in the natural world, this is only realistically possible in the simplest instances and therefore a different approach must be used (@Buckland2015). Researchers often use some form of sampling method to conduct a sample of the target population and draw conclusions for the overall population based on this sample.

The one of the most basic forms of sampling is plot sampling, with sampling designs being focused around either line transects or point transects. This comes alongside the assumption that every animal within each plot is observed.

Line transect sampling consists of a set of lines being placed over a study area by some predetermined method, for example systematic with regular spacing between each or randomly spaced, with both having a random starting point. The observer moves along each line, known as a transect, looking for animals or animal groups, referred to as clusters. These are defined by @Buckland2015 as "a group of animals with a well defined location for the group centre." For any animal or cluster the observer detects, they estimate or calculate the perpendicular distance _x_ of the animal or cluster from the nearest point of the line.

For point transect sampling, the transects are a set of points placed over the study area, with different placement methods available as with line transects. Most common is a systematically spaced grid with a random start point, as this provides the best coverage over the entire study region. At each point, the observer records any individuals or clusters observed from the point, along with the distance _r_ from the point at which the observation was made.

For this, the plots in line transect sampling are rectangles of dimension $2wl$, where _l_ is the length of a given transect and may change between transects depending on the survey design and area shape, and _w_ is the truncation distance. This is the distance from the line, beyond which observations are not recorded if the truncation distance is determined prior to the study. However, if the truncation distance is determined during the analysis phase, then this is the distance beyond which the observations are excluded from the analysis. For the plots in point transect sampling, these are circles of area $\pi w^2$ with the plot radius _w_ being the truncation distance.

These different designs are then used to calculate the animal density _D_ and therefore the overall abundance _N_ = _DA_, where _A_ represents the area of the study region. This is done by initially taking the total number of animals observed _n_ and dividing by the area of the plots _a_ to give $\hat{D} = n/a$. This then leads to the abundance estimate for the overall study area being $\hat{N} = \frac{nA}{a}$. For line transects, if we have k lines in total, then from above we have that $a = 2wL$ where $L = \sum_{i = 1}^k l_i$ is the the sum of the individual transect lengths $l_i$. This leads to the overall abundance estimate being:
$$ \hat{N} = \frac{nA}{2wL} $$
And, if we say have k point transects,from above $a = k\pi w^2$ leading to the abundance estimate of:
$$ \hat{N} = \frac{nA}{k\pi w^2} $$

## Distance Sampling

The one of most widely used methods of sampling for ecological populations is distance sampling, where information on the detectability of animals comes from the distances at which observations are made.

Distance sampling was first introduced by @Buckland1993 and includes a variety of designs such  as line and point transect sampling, which can then be used to estimate animal abundance using information on the distances to the individuals or clusters observed. The underpinning theory is that if the probability of animal detection can be estimated based on the sample observed, this can allow for estimates on how many animals were not observed and can therefore correct the abundance estimates to take this information into account. 

Distance sampling can therefore be thought of as a method of plot sampling, with the additional factor of not every animal on the plots being observed. The two simple designs of distance sampling are the same as those for plot sampling, with line transect sampling and point transect sampling designs available.

To take into account the fact that not all animals or clusters within each transect are observed, the probability of detecting an animal with a transect must be estimated. This is done by using the distances to the animals observed to fit a detection function.


## Detection function {#detfunc}

The detection function is estimated using the distances to the animals observed, _x_ for line transects and _r_ for point transects, to fit a function $g(x)$, defined as "the probability of detecting an animal that is distance _x_ $(0\le x \le w)$ from the line" (@Buckland2015). This can be similarly defined as $g(r)$ for point transects where _r_ $(0\le r \le w)$ is the distance from the point.

The normal technique is to assume that all animals on the point or line are definitely observed, such that $g(0) = 1$ and the observations are scaled to account for this. An example of a fitted detection function is seen in Figure \@ref(fig:exdet) . A further assumption is that the population is uniformly distributed about the transect, signified by the blue line in Figure \@ref(fig:exdet). The scaled observations are then plotted and a detection function fitted to these, representing the observed distribution of animals. The aim of this is to establish the proportion of animals observed in the study, allowing for those not observed to be accounted for. Those not observed are represented by the difference between the blue box and the red detection in Figure \@ref(fig:exdet) from @Workshop. 
```{r exdet, out.width='75%', fig.align='center', fig.cap= 'Example of a detection function is fitted to the observed distances. Blue box represents the assumed distibution of animals, uniform about the transect. Red curve represents a detection function fitted to the observation data ' }
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/fitted detect.jpg")))
```



Taking the detection function into account, we can then correct our abundance estimates from above to account for those animals not observed. This is done by defining $P_a$ as the expected proportion of animals observed within the study area. In the case of plot sampling above, $P_a = 1$ while for distance sampling $0 < P_a < 1$. $P_a$ can then be estimated based on the detection function $g(x)$ using an estimate $\hat{g}(x)$, , with a visualisation provided in Figure \@ref(fig:detline). This gives an estimate of $P_a$, $\hat{P_a}$ for line transects provided by the formula:
$$ \hat{P_a} = \frac{\int_0^w \hat{g}(x) dx}{w}$$
```{r detline, out.width='50%',fig.align='center', fig.cap= 'The probability Pa that an animal within distance w of a line is detected may be represented by the proportion of the rectangle that is under the curve in these plots. If the curve is the detection function g(x), this gives Pa = u/w where u is the area under the curve, represented by the integral from 0 to w of g(x), and the area of the rectangle is g(0).w = w, as g(0) = 1. (Buckland et al. 2015)'}
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/detection line.jpg")))
```

While for point transects:
$$ \hat{P_a} = \frac{2}{h(0)w^2}$$
where h(0) is the slope of the probability density function for detection distances _r_ , which is proportional to $g(r)r$, the detection function multiplied by _r_. A visualisation of this is contained within Figure \@ref(fig:detpoint) This leads to abundance estimates from above becoming:
$$ \hat{N} = \frac{nA}{2wL\hat{P_a}} $$ for line transects and $$ \hat{N} = \frac{nA}{k\pi w^2 \hat{P_a}} $$ for point transects.

```{r detpoint, out.width='50%', fig.align='center', fig.cap='The probability Pa that an animal within distance w of a point is detected may be represented by the proportion of the triangle that is under the curve in this plot. The area under the curve is one, as the curve is a probability density function. The triangle has base w and height h(0)w where h(0) is the slope of the hypotenuse, so that its area is h(0)w2/2, giving Pa = 2/[h(0)w2]. Note that, if all animals out to distance w were detected, then the probability density function would be represented by the triangle, and we would have Pa = 1 (Buckland at al. 2015)'}
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/detection point.jpg")))
```

\pagebreak
## Distance Sampling Simulations {#dsims}

In order for a simulation for a particular design being run, a number of objects must be first be specified. The first object is the study region, this can either be the default generated by R or user defined from a shapefile. Following this, a spatial distribution or density surface is required, from which animal locations can be generated based on the population description. The desired population size can be user defined and set for a series of simulations or be generated based on the spatial distribution supplied by the user. The desired truncation distance must then be defined and based on this an appropriate design can be generated (@Buckland2015).

The main considerations when constructing the design are the type, either line or point transects and the desired number or length of transects. If line transects are used, the design angle may be altered from its default of 0$^\circ$. The design angle is the orientation of the particular design, with the default of 0$^\circ$ representing a vertical orientation.

A further parameter that must be set it whether plus or minus sampling should be used. Plus sampling is where the transects extend beyond the survey region into a buffer zone of distance _w_ from the edge, however only observations within the survey region are recorded. With minus sampling, transects end at the edge of the study region and do not extend beyond. Minus sampling will be used for all simulations throughout the remainder of this report. This is because in reality, most studies conduct minus sampling due to the additional cost associated with running a plus sampling survey, in addition to the fact that _dsims_ does not support plus sampling simulations at the present time.

Based on the design, a realisation of the survey transects can be generated, during which the detection process is simulated. The user can then define a detection function, based on either a half normal ('hn') or hazard rate ('hr') with a defined scale parameter and the desired truncation distance _w_, with the hazard rate function also requiring a shape parameter. Examples of these can be seen in Figure \@ref(fig:detect):
```{r detect,  fig.show = 'hold', out.width='50%', fig.cap='Examples of different detection functions with a truncation distance of 50: Left: Half Normal with scale parameter 25, Right: Hazard Rate with scale parameter 25 and shape parameter 5'}
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/hn_detectfunc.jpg")))
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/hr_detectfunc.jpg")))
```

Therefore for an animal at distance _x_ from the closest transect the probability of the animal being detected is given by the detection function evaluated at _x_, provided _x_ is less than or equal to _w_.

The distance data generated during a realisation of a survey is then analysed to estimate the abundance _N_ of the study area. This is done by fitting a detection function to the observed distances from the, with the user specifying the different potential models to be fitted, with a model selection criteria used to select the best, using AIC as the default (@Buckland2015) This could be the same form as that specified during the detection process or different, with the option for covariates to be included.

At this stage, the encounter rate variance estimator can be specified, with a comprehensive summary of the various estimators found within @Fewster. Unfortunately, not all of the estimators mentioned within @Fewster are currently compatible with the survey realisations produced by _dsims_, with those compatible being 'R2','R3','O2', 'O3', 'P2' and 'P3'. The default estimator is 'R2' for line transects and 'P3' for point transects and unless otherwise stated, these were used within all simulations conducted in this report. @Fewster does however recommend the 'O2' estimator when using systematic parallel line transects, which will be investigated later in this report, however this estimator is only currently implemented for systematic parallel line designs.
If we look at how these estimators are constructed, it is possible to identify where one might be an improvement over the other. To begin assessing the variance, we revisit the density estimate as mention above, namely:

$$ \hat{D} = \frac{n}{\hat{P_a}} \times \frac{1}{2wL} =
\frac{1}{2w} \times \frac{n}{L} \times \frac{1}{\hat{P_a}} $$
Following on from this, the delta method is used to estimate the variance in the density, noting that $1/2w$ is a constant:

$$ [cv(\hat{D})]^2 = \bigg [ cv \bigg (\frac{n}{L}\bigg )\bigg]^2 + \bigg [ cv \bigg (\frac{1}{\hat{P_a}}\bigg )\bigg]^2$$

@Fewster notes that the encounter rate component of the variance, var(n/L) usually comprises around 75% of the total variance, while an estimate of var($\hat{P_a}$) can be obtained using standard likelihood theory. Variance estimators are required for n/L, as inference based upon a probability distribution is not likely to be sufficiently robust when dealing with inhomogeneous biological populations. This leads to estimators being used which are constructed around the empirical variability in each transects encounter rate, $n_i/l_i$. We first look at the construction of the 'R2' estimator. This is a design based estimator for random designs which arises as a result of a Taylor series expansion n/L about $\rho$, where  $\rho = E(\bar{n})/E(\bar{l})$ where $\bar{l}$ denotes L/k. This leads to the estimator becoming:
$$
\begin{aligned}
\widehat{var}_{R2}(\frac{n}{L}) &= \frac{k}{L^2}\times \frac{1}{k-1} \sum_{i = 1}^{k} (n_i - \hat{\rho}l_i)^2\\
        &=  \frac{k}{L^2(k-1)} \sum_{i = 1}^{k} l_i^2(\frac{n_i}{l_i} - \frac{n}{L} )^2\\
\end{aligned}
$$

which @Fewster notes is biased within the design framework. When using systematic designs, these often exhibit greater precision for the encounter rate than those of random designs, leading to additional information which could be included within the variance estimate. However, this information is not taken advantage of if the estimator assumes a random line placement, as in 'R2', leading to the likely overestimation of the variance. To account for this increased precision, we can make use of the systematic nature of the design by grouping neighbouring transects in a post-stratification technique and then calculate the individual variances of each strata, before combining to give the overall estimate. To begin explaining this, we look at the 'S2' estimator, while noting this is not currently working within _dsims_. This if formed by splitting the transects into strata, where each transect can only be in one strata, with the strata labels being _h_ = 1, ..., H, and letting there be $k_h$ lines within stratum _h_. This implies:
$$ var(\frac{n}{L}) \approx \frac{1}{L^2} \sum_{h = 1}^{H} k_h \text{var}_h(n_{hj} - \rho l_{hj} )$$

where $n_{hj}$ and $l_{hj}$ are individual random variables $n_i$ and $l_i$ from within stratum _h_. The 'S2' estimator is then formed by combining the individual stratum estimates in an heuristic manner and adding a weighting factor to each, with this being the total line length for each stratum.

$$ \widehat{var}_{S2}(\frac{n}{L}) = \frac{1}{L^2} \sum_{h = 1}^{H} L_h^2 \widehat{var}_h(\frac{n_h}{L_h})$$ where $n_h$ and $L_h$ are the observation and line length totals for each strata, with $\widehat{var}_h$ being the within strata variance estimator.

As noted, this is not currently available within the _dsims_ package, however there is an analogous estimator which is available, 'O2'. This is constructed in a very similar way to the 'S2' estimator, however it makes uses of the systematic nature of the designs by having overlapping strata where the strata have a natural ordering, often by spatial proximity. In this case, the first strata contains lines 1 and 2, the second lines 2 and 3 and so on. This leads to a total of k-1 strata each with two lines in each and is described as an overlapping strata post-stratification scheme. The individual variances of the strata are then combined to give k-1 variance estimates, with @Fewster noting that while the degrees of freedom are not known, the approximation of k-1 is normally assumed. Due to the systematic construction of the strata, it is known at $k_h = 2$ and strata _h_ contains lines _h_ and _h+1_, leading to the new variance estimator becoming:

$$ \widehat{var}_{O2}(\frac{n}{L}) =  \frac{2k}{L^2(k-1)} \sum_{i = 1}^{k-1} \frac{(l_i l_{i+1})^2} {(l_i +l_{i+1})^2}\bigg (\frac{n_i}{l_i} - \frac{n_{i+1}}{l_{i+1}}\bigg )^2$$

Having selected the most appropriate or available variance estimator, all of the simulation elements are in place and the simulation can be run. This is done by generating a realisation of the survey based on the design, population and detection function specified before the analysis above is conducted, These operations are then repeated the specified number of times, say _R_, to obtain a set of simulations of animal distribution and survey design, alongside a corresponding set of estimates $\hat{N}$ of N. Typical values for _R_ are between 100 and 1000 (@Buckland2015). The estimates along with other information are then extracted from the survey data allowing mean estimates and confidence intervals alongside other statistics to be calculated.

### Simulation Output

This section will detail the results extracted from the simulations, what they represent and what range of values we are looking for in each statistic. The first result is the mean abundance estimate, representing the average of all the abundance estimates generated throughout the simulation. The ideal value of this the true population of the study region, as this is the quantity we wish to estimate. To measure the means proximity to the true population, the bias is calculated, representing the difference between the mean abundance estimate and the true population, as a percentage of the true population. The aim is for the design to have a bias less than 5%.

Next we look at the statistics aimed at quantifying the variance of the design. Firstly there is the mean standard error of the estimates, which represents the estimated variance of the design and is a indicator of precision. Lower values of this are desirable. However, this should also be close to the Standard deviation of the estimates, a representation of the true variance of the design. Low values of this are also desirable, however it is preferred the standard deviation of the estimates is lower than the mean standard error of the estimates, its estimator. This is because issues associated with underestimating the variance are more severe than overestimating it.

The mean coefficient of variation (CV) is a measure of the average dispersion of the data around the mean. This is calculated as by dividing the standard error of each estimate by the abundance estimate and taking the mean of all these values. As with the variance statistics, smaller values of the CV are desirable. The confidence interval coverage is obtained by extracting the upper and lower 95% confidence bounds from each run of the simulation and determining if the true abundance is within these bounds. This gives a measure on the probability of the design containing the true abundance within the bounds of its 95% confidence interval, with the ideal level being above 0.95. 

The Skewness and Kurtosis of the abundance will also be examined for some of the simulations. These give information by comparing the relative size of the two tails of the estimates and by measuring the combined size of the tails respectively. This will give information on whether more high or low estimates are to be expected, as well as the number of these estimates to be expected compared to the normal distribution. There is no ideal values for these statistics, however a skewness of 0 and kurtosis of 3 are those found in the normal distribution. A positive skewness indicates a longer right tail while a kurtosis of above 3 represents heavier tails than the norml distribution. 

\pagebreak

## Density Surface Modelling

Density surface modelling extends the technique of distance sampling by relating animal density to spatial covariates within the environment, such as topology or habitat. The goal of this is to provide more information to allow for more appropriate management of the environment and the animals within (@Hedley2004). This has lead to an increase in demand for spatial models, especially for analysing line transect data. These models, in addition to providing abundance estimates, allow for relationships between covariates and abundance to be investigated (@Miller2013). The advantage of using this method over distance sampling is reduced variance in abundance estimates. This is due to the model being able to explain the inter-transect variation, a large component of the distance sampling variance (@Miller2013). However, a disadvantage of this method is the risk of bias being introduced into the abundance estimates through improperly specifying the model (@Hedley2004).

Density surface modelling can be implemented using either a one or two stage approach. Using a two stage approach, the detection function is fitted first then subsequently fitting a spatial model, while the one stage approach leads to estimating the detection and spatial parameters simultaneously. @Miller2013 states that 'Generally, very little information is lost by taking the two stage approach' as transect width is comparably smaller than that of the study region. Therefore, provided the density does not differ appreciably across the width of the transect or within the point, no information is lost by the two stage approach, as detection distances don't provide information on the spatial distribution (@Miller2013). This may lead is issues occurring where the density of the species has significant variability at the transect level. However, one drawback of the two stage model is that, to accurately evaluate the model uncertainty, the uncertainty in both the detection function and the spatial models should be suitably combined. The delta method, as mentioned in Section \@ref(dsims), is currently implemented within the software to propagate the variance in the detection function and the spatial model into the variance of the abundance estimates (@Bravington2021). The remainder of this report refers to methods implemented using only the two stage approach.

Initially, the detection function must be fitted, with the specification being the same as mentioned in Section \@ref(detfunc) above. Following this, the density surface model can be fitted. To enable this to occur, the transect data must be separated into segments. This is easily done for point transects with each point being a segment however it more complicated for line transects.

With line transects, they must be split up into J segments of length $l_j$. It is normal from the segments to be approximately square, with dimensions of $2w \times 2w$ where _w_ is the truncation distance of the design (@Miller2013). From here, the segment areas enter the model as part of an offset, to allow for non-constant segment areas. This leads to the line transect segments having an area of 2$wl_j$ and the point transect segments an area of $\pi w^2$. In the model, the counts or abundances are using a generalised additive model (GAM) using the sum of the smoothed covariates. 

### Response models

The model used when the count per segment is used as the response is:

$$ \mathbb{E}(n_j) = \hat{p_j}A_jexp[ \, \beta_0 + \sum_k f_k(z_{jk} ) ]\,$$

Where $f_k$ are the smoothed functions of the covariates and $\beta_0$ is the intercept term. By multiplying the segment area $A_j$ by the estimated probability of detection $p_j$ this gives the effective area of the segment, acting as an offset to account for different segment areas. Where distance is the only covariate in the detection function, $p_j$ is constant across all segments and therefore $\hat{p_j} = \hat{p} \forall j$. The distribution of $n_j$ can then be modeled using an overdispersed Poisson, Negative binomial or Tweedie distribution. 

An alternative to using this is to use abundance estimates for each segment generated by distance sampling as the response. To do this, the response $n_j$ is replaced by an estimator of the abundance in each section, $\hat{N_j}$ where this is defined as: 
$$ \hat{N_j} = \sum_{r = 1}^{R_j}  \frac{s_{jr}}{\hat{p_j}}$$

Where $R_j$ is the number of observations in the jth segment and $s_{jr}$ is the size of the rth group observed, with this being 1 if only individuals are observed. As identified by @Buckland2015, this is an Horvitzâ€“Thompson-like estimator of the segment abundance, allowing for covariates to be included through $\hat{p_j}$. The fitted model then becomes:
$$ \mathbb{E}(\hat{N_j}) = A_jexp [ \, \beta_0 + \sum_k f_k(z_{jk} ) ]\, $$
Where the model follows the same three distributions as before. The main difference between these models is that the offset is now the physical area of each segment, as opposed to the effective area in the first model for $n_j$. The model selected to be implemented within the simulation was the count per segment model, as modelling actual counts is preferable to modelling estimates for abundance (@Miller2013).

To allow for a DSM to predict abundance, a series of prediction cells must be defined. These are not necessarily restricted to just the original study region, allowing for regions outside the study area to be predicted over. Each of the prediction cells must include the same covariates as specified in the DSM, including the area of each cell. Predictions can then be made for the abundance in each cell and by summing these over the whole region, an overall abundance estimate can be obtained. The size of the prediction cells may be specified by the user, however cells "smaller than the resolution of the spatially referenced data" do not have an influence on the abundance estimates produced by DSM (@Miller2013).

\pagebreak

# Simulation extentions

This section details the extensions I made to the dsims package Marshall (2021) as part of this project.

## Prediction Grid

The prediction grid, as required by the DSM, is constructed across the study region prior to the simulation beginning, with the resolution of the grid set at half the truncation distance of the design. As @Miller2013 noted, smaller cells sizes could be used but there is a limit since using cells smaller than the spatial data resolution will not have an effect on the abundance estimates provided by DSM, and there is also the computational increase resulting from smaller cell sizes. This can be specified prior to the simulation as it is only dependant on the study region and will not change between iterations of the simulation.

Existing code to construct this prediction grid can be found in @Souchay20 and this was used in early testing and simulations. However, it was noted that some of the R packages used in this code are due to be retired at the end of 2023, namely the _rgeos_ package (@R-rgeos). Therefore, the decision was made to rewrite this code using generic functions from the far more widely used Simple Feature (sf) package (@R-sf). The prediction grid is currently set-up to create the grid over the entire study region, to allow for a comparison with the distance sampling approach. However, this is not a restriction and any prediction grid could be specified, including areas outside of the study region. This would allow for simulations to predict the abundance on areas not specifically studied in the survey, which the distance sampling approach cannot do without increasing the size of the study region itself. This offers a distinct advantage for DSM over the distance sampling approach, however it may be possible to generate unrealistic abundance estimates through extrapolations outside the range of data used in the model (@Miller2013). 

## Segment Data

For each iteration of the simulation, the observation data and segment data must be extracted from the survey realisation. To do this, a function _generate.dsm.data_ was written which first extracts the observation data from the survey and subsequently splits into the segments required by the DSM approach, using a function _to_segments_ which was written for this explicit purpose.

For Point transect designs, each point is treated as its own segment. However, in the case of line transect designs, the lines are split to allow them to be modelled as points. Each line is split into segments of approximate length $2w$ with _w_ being the truncation distance, as suggested by @Miller2013 and each segment assigned its own unique sample label. Subsequently, the length of each segment is recorded as the segments effort value and the centre of the segment as its location. Polygons of each segment are then created using _st_buffer_, with _w_ being used as the distance. This leads to squares of approximately $2w \times 2w$ for line transects and circles of radius _w_ for points.

These allow the area of each segment to be calculated, a requirement for the DSM model. This is calculated using _st_area_ for both the squares and circles from the respective transect types, on the intersection between the polygons and the outer boundary of the study region. This ensures only areas within the study region are counted towards segment area and internal strata boundary's do not split segments. Failure to do this results in the areas of some segments being larger than they are in the survey, and as a result the DSM abundance estimate is smaller, since the prediction grid is only over the survey area.

Once the segment areas have been calculated, the segments can be linked to the observation data by allocating each observation to the nearest segment and giving this the respective segments sample label in the observation data, overwriting the original allocation. Additionally, the coordinate reference system of the observation data is set as that of the segments to ensure consistency when using different reference systems to the default. 

## Modeling

Based on this data, both a distance sampling model and density surface model are constructed, with the DSM modeling the counts against the smooth of spatial locations using a tweedie error distribution. This error distribution is used as it offers a greater degree of flexibility than the dispersed-poisson or negative binomial (@Miller2013). The formulas for both could be changed to include environmental or other covariates such as strata, allowing for different detection functions to be fit to each strata.

In the smoothed term, the degrees of freedom is restricted to the total number of transects, as without doing this, the simulation was prone to failing to fit a model. The limit for this is the number of segments in the model, which in the case of point transects is the total number of transects. While this could be far larger for line transects, this would increase the computational requirements beyond a reasonable level and without offering a significant improvement, added to the fact that this model will be run within a simulation which is already computationally intensive. The abundance estimates are extracted from both models and stored alongside other statistics from each including the standard errors and confidence intervals for each model.

All elements of the distance sampling simulation were kept as is, with the additions made to facilitate the implementation of DSM designed to ensure the generality of the code for potential expansion in future.

\pagebreak

# Simulation - Default Region

## Introduction

The aim of conducting this simulation to is to examine a very simplistic region and density surface using a number of the default functions and parameters supplied within the _dsims_ package. The results of this will provide an initial starting point from which further simulations can be conducted to examine areas of interest identified here.

## Methods

This simulation was set up using the default region generated by _dsims_ @R-dsims, a 500m by 2000m rectangle, with the truncation distance selected at 60m. A basic test density was then constructed for the region with high and low spots as seen  in Figure \@ref(fig:defregion) with relatively gently gradients. This was chosen as the aim of this simulation is to to test a very simplistic set-up and defining the density in this way aligns with this aim. 

``` {r defregion, crop = TRUE, fig.cap='Density surface used for the Default region'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default_density.jpg')))
```

A population description was constructed based on this density surface with a true population of 1000. A detection function for simulating detections during the survey realisations was specified as a half normal with scale parameter of 30 and was kept constant across all designs, producing the detection function seen in Figure \@ref(fig:defdetect). This is separate to the detection function which is fitted to the survey data during the analysis phase. Here, the model to be fitted was a half normal function with no covariates included. The encounter rate variance estimators for the analysis stage are set as their defaults, 'P3' for the point designs and 'R2' for both line designs. 

```{r defdetect, out.width='50%', fig.align='center', fig.cap='Detection function for the default region, a half normal with scale parameter of 30'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default_detect.jpg')))
```

The designs used are the only part which change between the different simulations, other than the variance estimators as necessitated. Both point and line transect designs were constructed, with a systematic point design being specified alongside two different line designs, a systematic parallel line design and a complementary zigzag line design. The point design aimed of 25 points while both line designs aimed for 12 transects. An example survey for each of the designs is displayed  in Figure \@ref(fig:defsurvey). .
```{r defsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example Surveys for the default region. Top: Point Transect design, Middle: Parallel line design, Bottom: Zigzag line design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey zigzag.jpg')))
```
1000 iterations were completed for each simulations, being on the upper end of those recommended by @Buckland2015. A realisation of a simulated survey, for the Systematic parallel line design is shown in Figure \@ref(fig:defreal)


\pagebreak

```{r defreal, fig.show = 'hold', out.height='75%', fig.cap='Example Survey realisation for the default region using the Parallel line design. Red dots highlight individuals not observed. Blue dots represent individuals observed. Dark Blue lines represent the transects and the shaded areas either side show the area of observation out to the truncation distance. Histogram below denoted the detection distance, upon which a detection function will be fitted'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/default example survey.jpg')))
```


\pagebreak
## Results

### Default Region Point design

`r default.point <- read.csv('Estimates/Default1000point.csv') `

For the initial default region with the point transect design, the histograms of both the distance sampling and DSM abundance estimates are displayed  in Figure \@ref(fig:histdefpoint). These plots show that the estimates for both the DSM and DS methods appeared fairly symmetrical around the true abundance, with a slight right skew present. If we now compare the means of the two approaches, the mean of DSM estimates was `r round(mean(default.point$dsm.est), 2)` and the mean of DS estimates was `r round(mean(default.point$ds.est), 2)` from Table \@ref(tab:resdefpoint). It can be seen that the mean of the DSM estimates appears further from the true population of 1000 than the mean of the distance sampling estimates.

```{r histdefpoint, fig.show = 'hold', out.width='50%' , fig.cap='Histograms of  abundance estimates for Default region using a point transect design: Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}


hist(default.point$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(default.point$dsm.est), col = 'red')

hist(default.point$ds.est,breaks = 50,
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(default.point$ds.est), col = 'red')

results.point <- read.csv('Results/results default 1000 point.csv')
results.point$X <- NULL
```

Table \@ref(tab:resdefpoint) provides statistics which allow us to compare the precision of the models as well as their ability to capture truth within their confidence intervals. We see in this case that the DSM method has a larger and positive bias compared to the smaller and negative bias of the DS method. This indicates the DS method produced more accurate results, however both are within the desired 5% limit. Mean Standard errors of the estimates is slightly lower for the DSM, denoting the method is slightly more precise, this being an estimate of the variance. It is also closer to the statistic it is an estimate of, the standard deviation of the estimates, the true variance. However, it is noted that the true variance is actually marginally lower for the DS approach. The DSM method produces a smaller Coefficient of variation, however the difference is marginal and is due to the slightly smaller variance estimate. Next, the confidence interval coverage of each method is examined. The results show that the DS approach performed better at `r results.point[6,2]` compared with `r results.point[6,1]` for the DSM method. Within this, the DSM coverage was below the 95% level which would raise some concerns regarding the suitability of the design for obtaining abundance estimate from density surface modelling. The positive skewness statistics confirms the right skew observed in the histograms in Figure \@ref(fig:histdefpoint), with the kurtosis implying heavier tails than the normal distribution. 

```{r resdefpoint}

results.point <- results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))

kable(results.point,
      caption = 'Default region results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

### Default Region parallel Line design
`r default.line <-  read.csv('Estimates/Default1000line.csv')`

Now examining the results of the line transect design, we see the histograms of the two estimates  in Figure \@ref(fig:histdefline). These plots are very similar to their point transect counterparts, with the mean appearing close to truth and a right skewness observed. Investigating this further, we find the mean of DSM estimates to be `r round(mean(default.line$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(default.line$ds.est),2)` from Table \@ref(tab:resdefline), with these estimates being very close to the true value.

```{r histdefline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of abundance estimates for Default region using a parallel line design: Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}

hist(default.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(default.line$dsm.est), col = 'red')

hist(default.line$ds.est,breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(default.line$ds.est), col = 'red')

results.line <- read.csv('Results/results default 1000 line.csv')
results.line$X <- NULL
```

From Table \@ref(tab:resdefline), it can be observed that both methods have very low bias, indicating they are both highly accurate in this case. Mean Standard errors of the estimates is markedly lower for the DSM, denoting the method is far more precise, this being an estimate of the variance. It is also closer to the statistic it is an estimate of, the standard deviation of the estimates, the true variance. It is noted that the true variance is actually marginally lower for the DS approach, however it is still massively overestimating the variance. The DSM method produces a smaller coefficient of variation, with this due to the slightly smaller variance estimate. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`, however this is directly related to the overestimate of variance by the DS approach. Despite this, both designs achieved the desired coverage of 95%. As with the point transect results, the positive skewness statistics confirms the right skew observed in the histograms in Figure \@ref(fig:histdefline), with the kurtosis implying heavier tails than the normal distribution.

``` {r resdefline}

results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'Default region results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

### Default Region Zigzag Line design
`r default.zig <-  read.csv('Estimates/Default1000linezigzag.csv')`

Now examining the results of the zigzag line transect design, the histograms of the abundance estimates are displayed  in Figure \@ref(fig:histdefzig) These plots differ slightly to their point and parallel line counterparts, with the mean appearing further away from truth than previously as well as a more rounded shape then before. Investigating this further, we find the means of DSM estimates to be `r round(mean(default.zig$dsm.est),2)` and the mean of DS estimates to be `r round(mean(default.zig$ds.est), 2)` from Table \@ref(tab:resdefzig).

```{r histdefzig, fig.show = 'hold', out.width='50%' , fig.cap='Histograms of  abundance estimates for Default region using a zigzag line design: Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}

hist(default.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(default.zig$dsm.est), col = 'red')
hist(default.zig$ds.est,breaks = 50,
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(default.zig$ds.est), col = 'red')
results.zig <- read.csv('Results/results default 1000 zigzag.csv')
results.zig$X <- NULL
```

From Table \@ref(tab:resdefzig), it can be observed that both methods have a bias below the desired 5% level. However the bias from the DSM approach has, like the point transect results, a larger and positive bias compared to the smaller and negative bias of the DS method. This indicates the DS method produced more accurate results. Mean Standard errors of the estimates are again markedly lower for the DSM, denoting the method is far more precise, this being an estimate of the variance. It is also closer to the statistic it is an estimate of, the standard deviation of the estimates, the true variance. It is noted that the true variance is actually lower for the DS approach, however it is still massively overestimating the variance in comparison to the DSM approach. The DSM method produces a smaller mean coefficient of variation, as a direct result of lower variance estimates.  The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.zig[6,2]` compared to `r results.zig[6,1]`, however this is directly related to the overestimate of variance by the DS approach. Despite this, both designs achieved the desired coverage of 95%. The positive skewness statistics indicates a slight right skew, the degree of this is less than seen in either the point of parallel line results. As observed in the histograms in Figure \@ref(fig:histdefline), the kurtosis statistic implies lighter tails and a more rounded distribution than those seen previously. 

```{r resdefzig}


results.zig <- results.zig %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zig,
      caption = 'Default region results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Zigzag Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "HOLD_position")
```

## Discussion of Results

Overall it can be seen that the DSM method performed well in comparison the the DS method across all of the simulations. It is noted that the results above indicate the bias for DSM is more generally positive as opposed to the negative bias from DS, indicating that DSM appears more likely to overestimate the true abundance. Also, the result of variance estimation show that for the point design, the variance estimates are comparable between DS and DSM. However for both the parallel and zigzag line designs, DS greatly overestimated the variance, indicating the DSM approach was more precise.


\pagebreak

# Simulation - North Sea {#NS}

## Introduction

Having gone through a basic simulation using the default region produced by _dsims_, a more complex example was created to further test the capabilities of the simulations with a real world example. This will allow for a more complex region to be used, as well as examine how the density surface model performs when an unequal effort design is used, created through the stratification of the region. This region with be in the area of the North Sea off the East coast of the UK. As this would likely be a shipborne or airborne study, point transects are not realistic in this scenario, however they will be included for completeness.

To assess the performance of the distance sampling and density surface modelling approaches, both point and line transect designs will be examined. There will be a total of 6 different designs, one single point transect designs and two line transect designs will be used on both the original and stratified regions. The standard designs will be compared first before examining the stratified designs.

## Non-Stratified Designs

### Methods {#NSmeth}

As with the default region simulation, a systematic point design will be used alongside two line designs, systematic parallel line design and a complimentary zigzag line design, with all designs used a truncation distance of 10km.  The density maps for stratified and non-stratified will be different to justify the use of different strata, with density surface used for the Non-Stratified designs seen in Figure \@ref(fig:NSden) .
```{r NSden, crop = TRUE, fig.show='hold', out.width='75%',fig.align='center', fig.cap='Density surface for the basic North Sea regions'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North_Sea_density.jpg')))
```
A population description was constructed based on this density surface with a true population
of 1000. The detection function for simulating detections was kept identical between each design, with this being specified as a half normal detection function with a scale parameter of 5. This can be seen in Figure \@ref(fig:NSdet) . This is separate to the detection function
which is fitted to the survey data during the analysis phase. Here, the model to be fitted was
a half normal function with no covariates included. The encounter rate variance estimators
for the analysis stage are set as their defaults, â€˜P3â€™ for the point designs and â€˜R2â€™ for both
line designs.
```{r NSdet, out.width='75%', fig.align='center', , fig.cap='Plot of the detection function used in the basic North Sea regions: A half normal with scale parameter 5 and truncation 10'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Detect.jpg')))
```

Alongside the variance estimator, the designs are the only change between the simulations. To ensure sufficient observations to fit the detection function could be obtained, the number of samplers was differed between designs. While the covered area of each design would ideally be equal, to allow for comparison between designs, this was not possible in this case. This would have required both too many points alongside too few line transects. However, the coverage score for the line transect designs are approximately equal at 79, allowing these to be compared. The point transect designs aimed for 70 samplers in a systematic design while the line transect designs aimed for 25 samplers. The line transect designs consisted of a systematic parallel line design with transects perpendicular to the coast with a design angle of 90$^\circ$ and a complementary zigzag line design with a design angle of 0$^\circ$. An example of each design can be seen in Figure \@ref(fig:NSsurvey). 
```{r NSsurvey, fig.show = 'hold', out.width='45%', out.height='75%', fig.cap='Examples of surveys for the North Sea region. Top Left: Point Transect design, Top Right: Parallel line design, Bottom Left: Zigzag line design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey zigzag.jpg')))
```

5000 iterations were completed for each simulation. While this is more than the number recommended by @Buckland2015, time was not a constraint during the running of these simulations and this will give a wider range of possible results.

\pagebreak

### Non Stratifed Results - Point Design

`r NS.point <- read.csv('Estimates/North Sea5000point.csv')`

For the point transect design, the histograms of the abundance estimates from each simulation run are displayed in Figure \@ref(fig:histNSpoint). These plots suggest both methods are relatively good at estimating the true abundance of 1000, with both methods appearing to overestimate, with the DSM method overestimating slightly more than the DS method. There also appears to be a slight right skewness to both plots which will be looked at analytically. Investigating the means further, we find the mean of DSM estimates to be `r round(mean(NS.point$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(NS.point$ds.est),2)` as seen in Table \@ref(tab:resNSpt). These estimates are both close to the true abundance, with the overestimation present as mentioned above. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NS.point$dsm.est - NS.point$ds.est) > 0) / nrow(NS.point) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.point$dsm.est, NS.point$ds.est, paired = T, alternative = 'greater')
```
This returned a p-value of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. If we now compare the other variables extracted from the models in the below, we can examine the accuracy and variability of each method. 
```{r histNSpoint, fig.show = 'hold', out.width='50%', fig.cap='Histograms of the abundance estimates for the North Sea region using a point transect design: Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}


hist(NS.point$dsm.est,
     breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.point$dsm.est), col = 'red')

hist(NS.point$ds.est,
     breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.point$ds.est), col = 'red')

results.point <- read.csv('Results/results NS 5000 point.csv')
results.point$X <- NULL
```

From the results in Table \@ref(tab:resNSpt), it can be observed that in this case, the DS method overall performed better than the DSM method, having a smaller bias as well as higher confidence interval converge, despite having a smaller Mean Std error. Additionally, the standard deviation of the estimates is slightly less for the DS method, however both methods do a relatively good job at estimating this. Additionally, we observe from the skewness statistics that both methods returned a positive statistic indicating the right-skewed as identified above, which suggests the right tail is longer and this is supported by some of the estimates being over 2000, with the maximums for DSM and DS being `r round(max(NS.point$dsm.est),2) ` and `r round(max(NS.point$ds.est),2)` respectively. The kurtosis statistics for both methods are greater than 3, indicating we would expect more frequent outliers than a normal distribution. Therefore if a researcher assumed a normal distribution of these simulation estimates, they would see more extreme estimates than their assumed distribution. While the Mean CV for the DSM method is lower, the difference is marginal and does not out-way the bias or failure to achieve the desired confidence interval coverage of 95%. Overall this clearly seems to be a case where the DS method performs better than the DSM estimation method.

``` {r resNSpt}

results.point <-  results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.point,
      caption = 'North Sea Non-Stratified results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```


### Non Stratifed Results - Parallel Line Design

`r NS.line <-  read.csv('Estimates/North Sea5000line.csv')`

Now examining the results of the line transect design, we see the histograms of the two estimates  in Figure \@ref(fig:histNSline). These plots differ in comparison to their point transect counterparts, with less skewness apparent for these than is present with the point transect results. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NS.line$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NS.line$ds.est), 2)`, as highlighted in Table \@ref(tab:resNSline). Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey realisation `r sum((NS.line$dsm.est - NS.line$ds.est) > 0) / nrow(NS.line) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.line$dsm.est, NS.line$ds.est, paired = T, alternative = 'greater')
```
This returned a p-value of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.

```{r histNSline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of abundance estimates for North Sea Region using a parallel line design:  Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}


hist(NS.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.line$dsm.est), col = 'red')

hist(NS.line$ds.est,breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.line$ds.est), col = 'red')

results.line <- read.csv('Results/results NS 5000 line.csv')

results.line$X <- NULL
```

From the results in Table \@ref(tab:resNSline), it can be seen that in this case, the performance of both models was similar and each performed well in different statistics compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better variance estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`, however this is directly due to the higher Std errors from the DS method widening the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the size of the tails are very similar to those of a normal distribution. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation.
``` {r resNSline}


results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'North Sea Non-Stratified results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```


### Non Stratifed Results - Zigzag Line Design

`r NS.zig <-  read.csv('Estimates/North Sea5000linezigzag.csv')`

Now examining the results of the zigzag line design, we see the histograms of the two estimates in Figure \@ref(fig:histNSline). These plots are very similar to those of the parallel line design. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NS.zig$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NS.zig$ds.est), 2)`, as highlighted in Table \@ref(tab:resNSzig). Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NS.zig$dsm.est - NS.zig$ds.est) > 0) / nrow(NS.zig) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.zig$dsm.est, NS.zig$ds.est, paired = T, alternative = 'greater')
```
This returned a p-value of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS.
```{r histNSzig, fig.show = 'hold', out.width='50%', fig.cap='Histograms of abundance estimates for the North Sea Region using a zigzag line design:  Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}

hist(NS.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.zig$dsm.est), col = 'red')

hist(NS.zig$ds.est,breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.zig$ds.est), col = 'red')

results.zigzag <- read.csv('Results/results NS 5000 zigzag.csv')
results.zigzag$X <- NULL
```

From the results in Table \@ref(tab:resNSzig), it can be seen that in this case, the performance of both models was similar and each performed well in different statistics compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better error estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.zigzag[6,2]` compared to `r results.zigzag[6,1]`, however this difference is due to the higher standard errors from the DS method widening the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the size of the tails are very similar to those of a normal distribution. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 
``` {r resNSzig}


results.zigzag <-  results.zigzag %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))

kable(results.zigzag,
      caption = 'North Sea Non-Stratified results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```


## Stratified Designs

### Methods

For the stratified designs, it was decided to split the region into two strata, a large northern strata 'North' and a smaller southern strata 'South' to test how the simulation and both methods cope with a stratified design, where the effort is unequal across the study region. This situation could arise where a researcher knows in advance the regions of higher density and wishes to allocate more effort to covering this area. This is advantageous as it avoid the issue of using large amounts of effort in low-density areas with few detection (@Buckland2015). This was simulated by having far higher density in the 'South' strata and comparatively lower density in the 'North' strata. This can be seen in Figure \@ref(fig:NSSden).

```{r NSSden, crop = TRUE, fig.show='hold', out.height='50%',fig.align='center', fig.cap='Density surfaces for the basic and stratified North Sea regions'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North_Sea_Strata_density.jpg')))
```

A population description was constructed based on this density surface with a true population of 1000, where the population was split with 400 in the South strata and 600 in the North to account for the difference in density. The detection function for simulating detections was kept identical between each design and was the same as used in the non-stratified designs, being specified as a half normal detection function with a scale parameter of 5. This can be seen in Figure \@ref(fig:NSdet). This is separate to the detection function which is fitted to the survey data during the analysis phase. Here, the model to be fitted was a half normal function with no covariates included. The encounter rate variance estimators for the analysis stage are set as their defaults, â€˜P3â€™ for the point designs and â€˜R2â€™ for both line designs.

As with the non-stratified simulations, the only difference between the simulation is the design. Here, the point transect designs aimed for 60 samplers, with 30 in each strata, and the line transects for 25 samplers with 10 in the North strata and 15 in the South strata. This split places a large portion of the effort within the South strata, as desired. In terms of coverage scores, the line designs are comparable with each other, having an mean score of around 63, far less than the score of 79 for the non-stratified designs. The line designs consisted of a systematic parallel line design with transects perpendicular to the coast for both the North and South Stratum,using a design angle of 0$^\circ$. The zigzag line design selected has the transects in the North strata originating in the North of the Strata, with a design angle of 90$^\circ$, while in the South Strata, transects originate on the Western boundary with a design angle of 0$^\circ$. An example of each design, including the point design, can be seen in Figure \@ref(fig:NSSsurvey).
```{r NSSsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example surveys for each Stratified Designs. Top Left: Point Transect design, Top Right: Parallel line design, Bottom Left: Zigzag line design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey zigzag.jpg')))
```

The simulation was run 5000 times with the DSM model using a tweedie error distribution and the only covariates being spatial coordinates.

\pagebreak
### Stratifed Results - Point Design

`r NSS.point <- read.csv('Estimates/North Sea Strat5000point.csv')`

For the point transect design, the histograms of the abundance estimates from each simulation run are displayed  in Figure \@ref(fig:histNSSpoint). These plots suggest both methods are relatively good at estimating the true abundance of 1000, with both methods appearing to overestimate, with the DSM method overestimating slightly more than the DS method. There again appears to be a slight right skewness to both plots which will be looked at analytically below. Investigating the means further, we find the mean of DSM estimates to be `r round(mean(NSS.point$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(NSS.point$ds.est),2)` as seen in Table \@ref(tab:resNSSpoint). These estimates are both close to the true abundance, with the overestimation present as mentioned above and to a very similar level to the Non-Stratified point results. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.point$dsm.est - NSS.point$ds.est) > 0) / nrow(NSS.point) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NSS.point$dsm.est, NSS.point$ds.est, paired = T, alternative = 'greater')
```
This returned a p-value of `r test$p.value` indicating at any significance level, the null hypothesis of the difference between the DSM and DS estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. If we now compare the other variables extracted from the models below, we can examine the accuracy and variability of each method.
```{r histNSSpoint, fig.show = 'hold', out.width='50%', fig.cap='Histograms of abundance estimates for the Stratified North Sea Region using a point transect design: Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}

hist(NSS.point$dsm.est,
     breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.point$dsm.est), col = 'red')

hist(NSS.point$ds.est,
     breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.point$ds.est), col = 'red')

results.point <- read.csv('Results/results NS Strat 5000 point.csv')

results.point$X <- NULL
```

From Table \@ref(tab:resNSSpoint), it can be observed that in this case, the DS method overall performed better than the DSM method, having a smaller bias as well as higher confidence interval coverage, despite having a smaller Mean Std error. Additionally, the standard deviation of the estimates is slightly less for the DS method, however both methods do a relatively good job at estimating this. Additionally, we observe from the skewness statistics that both methods returned a positive statistic indicating the right-skewed as identified above, which suggests the right tail is longer and this is supported by some of the estimates being close to 2000, with the maximums for DSM and DS being `r round(max(NSS.point$dsm.est),2) ` and `r round(max(NSS.point$ds.est),2)` respectively. The kurtosis figures for both methods are greater than 3, indicating we would expect more frequent outliers than a normal distribution. While the Mean CV for the DSM method is lower, the difference is marginal and does not out-way the bias or variance issues of this method. Overall this clearly seems to be a case where the DS method performs better than the DSM estimation method.
``` {r resNSSpoint}


results.point <-  results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.point,
      caption = 'North Sea Stratified results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```


### Stratifed Results - Parallel Line Design
`r NSS.line <-  read.csv('Estimates/North Sea Strat5000line.csv')`

Now examining the results of the line transect design, on the same density surface, we see the histograms of the estimates  in Figure \@ref(fig:histNSSline). These plots are similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth. However, there appears to be less skewness for these than is present with the point transect results, exhibiting a slightly sharper point at the mean. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NSS.line$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NSS.line$ds.est), 2)`. These estimates are very close to the truth, with DSM again overestimating in comparison to DS. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.line$dsm.est - NSS.line$ds.est) > 0) / nrow(NSS.line) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS. `r test <- wilcox.test(NSS.line$dsm.est, NSS.line$ds.est, paired = T, alternative = 'greater')` This returned a p-value of `r test$p.value` indicating at any significance level, the null hypothesis of the difference between the DSM and DS estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS.

```{r histNSSline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of abundance estimates for the Stratified North Sea Region using a parallel line design:  Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates' }

hist(NSS.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.line$dsm.est), col = 'red')

hist(NSS.line$ds.est,breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.line$ds.est), col = 'red')

results.line <- read.csv('Results/results NS Strat 5000 line.csv')
results.line$X <- NULL
```
From Table \@ref(tab:resNSSline), it can be seen that in this case, the performance of both models was very similar throuhgout all areas, with the exception being the mean estimates and bias. The mean estimate for DSM was again a slight overestimation and as a result, bias was positive, while DS underestimated by almost the same amount, leading to an almost identical but negative bias. The Mean Std errors and Std Deviation of estimates were slightly lower for DSM, however the difference in both cases is very small. The Mean CV for each method is very similar, being slightly in favour of the DSM method although the difference is negligible. The DS method has a slightly higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`, however this small difference is likely due to the slightly higher Std errors from the DS method widening the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is small. The kurtosis scores are close to 3 but larger, suggesting the distribution of the estimates are is similar to that of a normal distribution, with a slight trend towards a leptokurtic distribution. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 

``` {r resNSSline}


results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'North Sea Stratified results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```


### Stratifed Results - Zigzag Line Design

`r NSS.zig <-  read.csv('Estimates/North Sea Strat5000linezigzag.csv')`

Unfortunately, for the stratified zigzag design, an error intermittently occurred within the simulation which caused the fitting of the density surface model to fail. Upon further inspection, this error did not occur within any of the code written as part of this project. After consultation with L Marshall, the author of the _dsims_ package, it was determined the error likely occurred as a result of how some of the survey realisations within _dsims_ were constructed. This likely led to issues with the coordinates for the transects and segments, with this not being an issue within the DS model however the density surface model would not accept these, resulting in the error. It is outside the scope of this project to investigate the root cause of this error, therefore L Marshall was notified of the bug and given access to the code to determine the root cause. As mentioned, this error occurred intermittently, resulting in a full set of 5000 iterations not being completed in a single run. However, multiple runs were completed with the exact same conditions until a total of 5000 were completed, with these results being examined below. Examining these results, we see the histograms of the two abundance estimates  in Figure \@ref(fig:histNSSzig). These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth.However, there appears to be less skewness for these than is present with the point transect results. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NSS.zig$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NSS.zig$ds.est), 2)`. These estimates are very close to the true Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.zig$dsm.est - NSS.zig$ds.est) > 0) / nrow(NSS.zig) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.zig$dsm.est, NS.zig$ds.est, paired = T, alternative = 'greater')
```
This returned a p-value of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS.

```{r histNSSzig, fig.show = 'hold', out.width='50%', fig.cap='Histograms of abundance estimates for the Stratified North Sea Region using a zigzag line design:  Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates' }

hist(NSS.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.zig$dsm.est), col = 'red')

hist(NSS.zig$ds.est,breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.zig$ds.est), col = 'red')

results.zigzag <- read.csv('Results/results NS Strat 5000 zigzag.csv')
results.zigzag$X <- NULL
```

From the results in Table \@ref(tab:resNSSzig), it can be seen that in this case, the performance of both models was similar and each performed well in different areas compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better error estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.zigzag[6,2]` compared to `r results.zigzag[6,1]`, however this difference is due to the higher Std errors from the DS method widening the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the kurtosis is very similar to that of a normal distribution. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation.

``` {r resNSSzig}

results.zigzag <-  results.zigzag %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zigzag,
      caption = 'North Sea Non-Stratified results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```


## Discussion of Results

The aim of this section is to briefly summarise the results of the above simulations, due to the extensive number of results. Initially looking at the non stratified results, those from the point design appeared very similar for both the DSM approach and the DS approach, with DSM having a slightly higher bias. While DSM was better at estimating the true variance, the variance estimate was larger than that of DS. Added to the failure to achieve 95% confidence interval coverage, the DS was concluded to perform better in this design. Examining the non-stratified line transect designs, both designs found similar results, with DSM having a larger and positive bias in comparison to the negative bias of DS for both. Due to the similar coverage scores, these designs can be compared. Consisted across both was DSM's ability to better estimate the true variance, leading to more precise estimates, at the expense of confidence interval coverage. The variance estimates across the two designs are very similar, however the true variance in the zigzag design is slight higher than that of the parallel line design. This indicates that while the true variance is higher, both methods do a better job of estimating it that with the true variance in the line design.

Now looking at the stratified result, the point transect results are very similar to those of the non-stratified design. Obvious difference is that the precision of the design has decreased with the variance estimates being higher, however this is likely due to the lower coverage of the stratified design. Again DSM appears better at estimating the true variance. The distributions of the estimates in both designs is similar across both point designs, with a visible right skew present in each of the histograms, supported by the skewness statistics. Examining the line transect designs, both designs offer similar levels of accuracy, each having low levels of bias. Interestingly for variance estimation, the switch to stratified design has greatly improved the estimation for the DS method, bringing it comparable to that of DSM and even better in the case of the zigzag design. This implies stratification may be a possible solution to the previously identified variance overestimation of the DS approach.

\pagebreak

# Simulation - Buckland 2015 Example

## Introduction

For our next set of simulations, I will revisit the the example simulation provided in Section 2.5.2 of @Buckland2015 and use both a DS and DSM approach to repeat these simulations. It must be noted that the original analysis was part of a survey design case study by L Marshall and used the R package _DSsim_ @R-DSsim . For the purpose of this study, each of the designs and simulations will be translated into the _dsims_ @R-dsims package used throughout the remainder of the simulations to ensure comparability with the DSM approach developed, with all details from the original designs kept constant. The purpose of this simulation is to conduct a comparison between different designs with comparable cost, as would be conducted using the existing simulation set-up. This region also has the added benefit of verifying the simulation method developed for DSM can be run on study regions with internal holes, such as islands on a seaborne survey.

## Methods

The study region for these simulations is an area of the Baltic Sea, off the South East coast of Denmark and located directly to the north of Kiel, Germany. The density surfaced used is displayed alongside the detection function in Figure \@ref(fig:Montdensdetect), where a half-normal detection function with a scale parameter of 500 will be assumed. The population description was constructed based on this density surface, with the true population fixed at 1500 individuals and using a truncation distance of 1000m.  
```{r Montdensdetect, fig.show = 'hold', out.width='50%', fig.cap='Density surface and detection function for Buckland 2015 example. Detection function is half normal with truncation 1000m and scale parameter 500'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave density.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave detect.jpg')))
```
As with previous simulations the detection function which is fitted to the survey data during the analysis phase was a half normal function with no covariates included. The encounter rate variance estimators for the analysis stage are set as their defaults, â€˜P3â€™ for the point designs and â€˜R2â€™ for both line designs.

Three different designs will be used, being the same as those tested in the case study in @Buckland2015. These will be a systematic parallel line design, a random parallel line design and a zigzag design. Examples of each design are displayed in Figure \@ref(fig:Montsurvey). The random design aimed for 15 samplers at a design angle of 45$^\circ$, with the systematic parallel line design using the same design angle and a transect spacing of 12000m. These were chosen to remain consistent with the original case study in @Buckland2015. The specification of the zigzag design is not covered within supplementary details of the original study, however experimentation resulted in a design with 15 transects with a design angle of 135$^\circ$ bounded within a complex hull being chosen. This was selected as it appeared a good fit to the examples presented in @Buckland2015 and ensured that the cost of the survey, measured by the track line length, was comparable across the designs. 

\pagebreak

```{r Montsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example surveys for each Design: Top Left: Random Parallel Line, Top Right: Systematic Parallel line, Bottom Left: Zigzag line'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey random line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey parallel line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey zigzag.jpg')))
```
As with each of the simulations run as part of the North Sea section, 5000 iterations will be run for each design. While this is different from the 100 iterations originally conducted, the increase in the number of iterations will increase the reliability of the results.

\pagebreak

## Results - Random Parallel Line

`r Mont.rand <-  read.csv('Estimates/Montrave5000linerandom.csv')`

We can now examine the results of the Random parallel line design by initially plotting histograms of the estimates from both the DS and DSM methods  in Figure \@ref(fig:histMontrand). While the plot for the DS estimates appears similar to those we have seen before, the estimates for the DSM method appear markedly different. This is due to the presence of very extreme overestimates with the largest estimate being `r round(max(Mont.rand$dsm.est))`, over 3 times the true population. In addition to this, `r sum(Mont.rand$dsm.est > max(Mont.rand$ds.est))` of the DSM estimates were greater than the largest DS estimate of `r round(max(Mont.rand$ds.est))`  It initially appears that the majority of the DSM estimates are centred around the true population of 1500, however this will be investigated further below. As with the previous simulations, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((Mont.rand$dsm.est - Mont.rand$ds.est) > 0) / nrow(Mont.rand) *100` % of the time, indicating it is likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(Mont.rand$dsm.est, Mont.rand$ds.est, paired = T, alternative = 'greater')
```
This returned a p-value of `r test$p.value` indicating at any significance level, the null hypothesis of the diffeence between the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS.

```{r histMontrand, fig.show = 'hold', out.width='50%', fig.cap='Histograms of abundance estimates for Buckland 2015 example region using a random parallel line design: Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}

hist(Mont.rand$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(Mont.rand$dsm.est), col = 'red')

hist(Mont.rand$ds.est,breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(Mont.rand$ds.est), col = 'red')

results.rand <- read.csv('Results/results Montrave random.csv')
results.rand$X <- NULL
```

In Table \@ref(tab:resMontrand), we can see the various statistics extracted during the course of the simulations. As we have seen throughout throughout this report, the mean for the DSM is a slight overestimates and the DS is a slight underestimates. This is transferred into the bias, with the DSM having a positve bias and DS having a negative bias, however both of these are below the 5% level so no concerns are raised for either method. Unfortunately, there are major concerns regarding the Mean Std errors of the estimates, which are the estimator of the Std deviations. For both methods, it can be seen that they are underestimates of the Std deviations and very severely in the case of the DSM method. This is problematic as not capturing all of the variance can lead to the the results assuming they are more accurate than they actually are. These underestimations of variance are translated into the confidence interval coverage, where both fail to contain truth at the 95% confidence level. Additionally, the skewness and kurtosis statistics are very different for the DSM method, likely as a result of the extreme overestimations produce by some survey realisation. Overall, it could be concluded that in this scenario, the DS method performed better, by virtue of underestimating the variance less and having less extreme estimates, however neither method should be considered adequate in this case.

``` {r resMontrand}


results.rand <-  results.rand %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.rand,
      caption = 'Buckland 2015 example results - Random Line') %>%
        add_header_above(c('Statistic' = 1, 'Random Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "HOLD_position")
```

\pagebreak
## Results - Systematic Parallel Line

`r Mont.para <-  read.csv('Estimates/Montrave5000lineparallel.csv')`

Moving on the systematic parallel line design, histograms of the abundance estimates are plotted  in Figure \@ref(fig:histMontline). These plots in Figure \@ref(fig:histMontline) appear to be a significant improvement over those seen with the random line design. This can be determined based on the significant reduction in the range of estimates, especially in the case of the DSM approach. The mean of the estimates, also appears very close to truth at 1500, with some indication that the DSM approach has slightly overestimated. A more detailed analysis of the simulation results can be seen in Table \@ref(tab:resMontline).
```{r histMontline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of adundance estimates for Buckland 2015 example region using a systematic parallel line design : Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}


hist(Mont.para$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(Mont.para$dsm.est), col = 'red')

hist(Mont.para$ds.est,breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(Mont.para$ds.est), col = 'red')

results.para <- read.csv('Results/results Montrave parallel.csv')
results.para$X <- NULL
```

From the results in Table \@ref(tab:resMontline) the overestimation implied from the histograms is clearly seen, with the mean of the abundance estimates having a bias of almost 2.5%. However, when it comes to the variance estimation, the DSM approach is far superior in this case. While the true variance, the Std Deviation of the estimates, is lower for the DS approach, the estimate of this, the Mean Std error of the estimates is dramatically larger than what it is trying to estimate. In contrast the DSM variance estimate is very close to the true variance. These differences in variance estimation the propagate into the coefficients of variation, with again the DSM method having a lower CV. These also have an effect on the confidence interval coverage, with the DS results capturing truth over 99% of the time. However, this is most likely due to the high variance estimate for this approach widening the confidence intervals. Despite the DSM method having a lower confidence interval coverage, both methods achieved coverage at the 95% level resulting in no cause for concern with either method. The Skewness and Kurtosis statistics are near identical for both sets of abundance estimates and appear close to those of a normal distribution.

``` {r resMontline}


results.para <-  results.para %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.para,
      caption = 'Buckland 2015 example results - Parallel Line') %>%
        add_header_above(c('Statistic' = 1, 'Systematic Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```


## Results - Zigzag Line

`r Mont.zig <-  read.csv('Estimates/Montrave5000linezigzag.csv')`

The final simulation run as part of the case study in @Buckland2015 used a zigzag design, the results of which are displayed below. In Figure \@ref(fig:histMontzig) the histograms of the abundance estimates for the DSM and DS approaches can be observed. These appear to follow a similar pattern to those seen in the systematic parallel line simulation above. Namely both histograms look fairly pointed, with the mean for the DSM estimates appearing to be a slight overestimate while the mean for the DS abundance estimates looks to be close to truth. The results are examined more in-depth in Table \@ref(tab:resMontzig).

```{r histMontzig, fig.show = 'hold', out.width='50%', fig.cap='Histograms of abundance estimates for Buckland 2015 example region zigzag line design: Left- DSM approach, Right- DS aproach. The red line in each denotes the mean of each set of estimates'}

hist(Mont.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM abundance estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(Mont.zig$dsm.est), col = 'red')

hist(Mont.zig$ds.est,breaks = 50, 
     main = 'Histogram of DS abundance estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(Mont.zig$ds.est), col = 'red')

results.zig <- read.csv('Results/results Montrave zigzag.csv')
results.zig$X <- NULL
```

The results seen within Table \@ref(tab:resMontzig) appear very similar to those seen for the systematic parallel line design in Table \@ref(tab:resMontline). Once again, the DSM approach has slightly overestimates the abundance, with a bias of over 2%, however this is below the level of 5% at which concerns could be raised. In contrast, the DS approach slightly underestimates the true abundance, having a bias of below 1%. The similarities continue into the variance estimates, where DSM does a far better job of estimating the true variance than the DS method. This leads to the DSM results having a lower coefficient of variation, a desirable attribute. However, when investigating the confidence interval coverage, the DS approach, as usual, perform better, with truth being within the confidence interval over 99% of the time, although, as has been noted previously several times, this is likely due to the large variance estimate. In contrast, the coverage from the DSM method is just below 95% raising some elements of concern about its ability to cover truth within the confidence interval. The skewness and kurtosis statistics agree with the initial observations of the histograms, with the kurtosis suggesting a slightly leptokurtic distribution, one with larger tails and more pointed in the centre than that of the normal distribution, while the skewness suggests a slight right skew to the estimates, indicating a bias towards higher estimates. Overall, it could be concluded that the DSM approach proved superior in this case, mainly due to the accuracy of the variance estimation over the DS approach. While the Bias and confidence interval coverage were not as good as those for the DS method, I would not consider them sufficiently concerning to out-way the improvement in variance estimation.

``` {r resMontzig}

results.zig <-  results.zig %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zig,
      caption = 'Buckland 2015 example results - Zigzag Line') %>%
        add_header_above(c('Statistic' = 1, 'Zigzag Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```


## Discussion of Results

As both the Systemic parallel line design and the zigzag design were selected and specified such that they had a comparable cost to the survey. This cost is the trackline length, which is to total distance required to travel along each transect and includes the distances between transects. This is often termed the cost of a study as the distance travelled during a study is a major factor in the overall cost of a study, such as through fuel costs for air or seaborne studies as well as the time and resources associated with additional distances travelled. In these designs, the mean trackline length was around 700km for both survey designs. This allows for the results of the two designs to be compared and would be one of the steps conducted when selecting an appropriate survey design in the lead up to a real work survey. Comparing the results in both Tables \@ref(tab:resMontline) and \@ref(tab:resMontzig), it can be seen that the mean estimates from the DSM method are consistent accross the two survey designs, with a similar level of positive bias present in both. With the DS results, there appears to be a reduction in bias when using the parallel line design. Looking at variance estimation, the variance estimates from the zigzag designs are lower for both methods, indicating an increase in the precision as a result of using the zigzag design. The coefficients of variation stayed similar across the different designs as did the confidence interval coverage, with the skewness and kurtosis statistics also changing little between the two designs. Overall these results are consistent with those seen in @Buckland2015 with it being noted that the zigzag design offered a higher level of precision than the Mean Std error suggests. This has been confirmed by both the original DS method as well as the DSM approach, with this approach in fact giving improved estimates for the variance. It is however noted within @Buckland2015 that while the trackline length was similar for both designs, in situations where travelling on effort, i.e on a transect, is far more costly than off effort, the advantage gained from the zigzag design are negated by the addition cost. Despite this, in general, zigzag designs offer a higher level of efficiency due to the substantial increase in time spent on effort in comparison to parallel line designs. 

\pagebreak

# Simulation - Variance estimation tests {#ervar}

## Introduction

As has been noted through the analysis of previous simulations, the variance estimation of the DS method has often performed poorly in comparison with that of the DSM approach. As noted within Section \@ref(dsims), the default 'R2' encounter rate variance estimator has been used throughout all of the simulations so far, as according to @Fewster this shows good performance for random line designs. However, many of the simulations run have used systematic line designs, for which it is noted that 'R2' may not perform well. In this case, @Fewster recommend using the 'O2' estimator. Therefore, the aim of this simulation is to compare the variance estimates achieved using the default 'R2' estimator and the recommended 'O2' estimator on a suitable design.

## Methods

To investigate if changing the variance estimator improves the performance of the DS method, we return to the non stratified North Sea region in Section \@ref(NS). Here, the simulation will follow a systematic line survey as used previously, seen in Figure \@ref(fig:NSsurvey) alongside the density surface seen in Figure \@ref(fig:NSden). All specific details of the survey design, as mentioned in Section \@ref(NSmeth), are kept constant, with the detection function being a half normal function with a scale parameter of 5, as seen in Figure \@ref(fig:NSdet). The only difference between the two simulations is during the specification of the distance sampling analysis, with the encounter rate variance estimator changing between the two design. One design used the 'R2' estimator and the other used the more appropriate 'O2' estimator as recommended by @Fewster. 

## Results

From Table \@ref(tab:resR2est) the same pattern as has been seen throughout this report is evident. The variance estimate, the Mean std error, for the DS method is far larger than the true variance, the Std Dev of the estimates, by over 60%. In comparison, the variance estimate for the DSM approach is only a slight overestimate at only around 10% higher. These then transfer into the coefficients of variation with the DSM having a respectively lower CV than that of the DS method. The confidence interval coverage for the DS method is very impressive, however this is likely due to the large variance estimates widening the confidence interval appreciably to contain truth. While the coverage for the DSM method is slightly lower, it is still above the 95% confidence limit and does not raise concerns. Overall in this case the DSM method can be said to have performed better, mainly due to the substantially better variance estimate and resulting coefficient of variation improvements over the DS method. 

``` {r resR2est}
results.R2 <- read.csv('Results/results NS R2 estimator 5000.csv')

results.R2$X <- NULL

results.R2 <-  results.R2 %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.R2,
      caption = 'North Sea Line transect results with "R2" encounter rate variance estimator') %>%
        add_header_above(c('Statistic' = 1, '"R2" estimator' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

Moving on now to look at the results in Table \@ref(tab:resO2est), from when the variance estimator is set as the 'O2' estimator. It can be seen that there has been a significant improvement in the variance estimation in comparison to the 'R2' results above. The variance estimates for both methods are near identical now, with both being an overestimate of the true variance by around 10%. It appears that while the variance estimate for the DSM method is slightly closer than the DS approach, the difference is very marginal, highlighting the improvements to the estimation in the DS method. The confidence interval coverage for DS is slightly lower than previously, with this being the only statistic which appears worse for the 'O2' estimator than the 'R2' estimator. However this is likely due to the more representative variance estimate shrinking the confidence interval. Despite this, both methods acheived a confidence interval coverage above the 95% level. Overall in this case there is very little to differentiate between the two methods. It could be considered that the DS method performed better, as given that all other statistics are near identical, the mean estimate is closer to truth and as a result the bias is slightly smaller. However, the DSM method still performed well, with its bias being below 1%. 

``` {r resO2est}
results.O2<- read.csv('Results/results NS O2 estimator 5000.csv')

results.O2$X <- NULL

results.O2 <-  results.O2 %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.O2,
      caption = 'North Sea Line transect results with "O2" encounter rate variance estimator') %>%
        add_header_above(c('Statistic' = 1, '"O2" estimator' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

## Discussion of Results

These simulations have highlighted the improvements which can be made to the DS method to improve its variance estimation. It can be seen that the DS approach massively overestimates the variances using the default 'R2' estimator, while moving to the recommended 'O2' estimator improves this and makes it comparable or possibly better than the DS method, if the particular design allows. It is noted that if variance estimation can be improved, in the limit the DS approach will be more precise, due to the smaller true variance overall.


\pagebreak
# Discussion

This report has investigated a wide variety of simulations and scenarios, which have resulted in a number of intriguing results being discovered. Simulations on the basic default region of _dsims_ proved encouraging, with variance estimation comparable for point designs. However, distance sampling overestimated the variance for parallel and zigzag line designs, highlighting the improved precision of DSM noted by @Miller2013. Also present was a trend of DSM to overestimate the abundance. This continued throughout all of the simulations conducted, indicating the potential for a positive bias within DSM.

The simulation on the North Sea region provided further interesting results, indicating point designs may not be ideal for DSM, due to lower precision with a higher bias, in addition to having low confidence interval coverage despite higher variance estimates. However, for the non-stratified designs, the DSM results showed a marked improvement over those of DS, with DS greatly overestimating the variance, leading again to DSM having greater precision. However, the stratification of the region reduced the DS variance estimates in line with those of DSM, indicating a potential way of the variance estimates provided by DS.  

Simulations using the example for @Buckland2015 proved the most interesting, in particular for the random design. Here, both method underestimated the variance, with DSM doing so to a far greater degree. Additionally, DSM produced some extreme overestimates, the largest over three times the true population. This, coupled with poor confidence interval coverage, indicates DSM may be less well suited to random designs, due to the potential for extreme overestimates. The results of the other two designs, the systematic parallel line and zigzag line designs, mirrored those found in @Buckland2015, where the zigzag design would be selected due to its improved precision. Within each design, a similar pattern as before emerged with DS overestimating the variance leading to DSM having improved precision, despite the true variance being lower for DS in both cases. 

The final simulation focused on the common trend seen throughout, DS overestimating the variance in comparison to DSM. As a result of changing the encounter rate variance estimator, variance estimatin for DS was greatly improved and became comparable with DSM. However, it was noted this is only an option for a select number of designs.


## Limitations

While a number of different simulation have been conducted throughout this report, it is noted that these are fairly basic in nature, and only cover a small range of the full capabilities of both distance sampling and density surface modelling. Some of these are also not realistic for real world surveys, having more samplers and greater truncation distances than could be reasonable expected, resulting in very high coverage scores. While the central aim was to examine potentially interesting scenarios, testing the simulation method developed was also part of the decision behind the simulations selected, ensuring it is able to sufficiently deal with issues encountered early on in the development process. These issues were namely the creation of the segments required for the DSM, with the Buckland 2015 example introducing the most errors as a result of the islands presents which had to be overcome. To that end, there are a number of other areas which I wished to investigate, had time allowed, which can be split into two separate areas, variance estimators within _dsims_ and further more in depth simulations to determine the limits of the model and simulations.

## Further simulations

As noted within Section \@ref(dsims), there are a variety of different encounter rate variance estimators available for distance sampling data, as detailed within @Fewster. However, at present, only a select few are available within the _dsims_ package and even these are limited based on the particular design chosen. For example, while the 'O2' estimator tested during Section \@ref(ervar) proved to markedly improve the variance estimation, this estimator is only available within the existing simulations when using systematic parallel line designs, and not with the very common zigzag design. While distance sampling analysis can be conducted with variance estimators not available within the simulation, this allows for a potentially unfair comparison between survey designs when the best available estimator is used, as opposed to the best overall and may results in an non-ideal design being selected. As identified throughout the majority of the simulations, the DS approach did not perform as well as the DSM method with regards to variance estimation, highlighting this is an area where substantial improves could be made.

As identified during Section \@ref(NS), it was also found that stratifying the study region resulted in a marked improvement in variance estimation for the distance sampling method. While this is a more basic idea of the stratification used within the some of variance estimates suggested by @Fewster, this may be a temporary solution for designs such as zigzag which do not support these better variance estimators.

At the current stage, none of the simulations have looked at covariates within either the detection function or the models themselves. As seen within the example study conducted within @Miller2013, an example of this is using depth within seaborne surveys as a covariate within the models. Due to the likelihood of covariates being considered within both distance sampling and density surface modelling approached, an interesting set of simulations would be to look at how the use of covariates influences the results generates by each method and indeed if the coefficients of these vary between the approaches. At the current moment, the DSM simulations developed are not capable of implementing this, however, the additional work required would not be major and would involved ensuring any covariate information is appropriately passed to the relevant segments. The model could then be generalised to account for the additional information and allow these additional factors to be implemented.

Additionally, when discussing covariates and the segments, @Miller2013 suggests that the segment size for DSM should be chosen such that neither the density nor covariates should change substantially within the segment. A potentially interesting comparison could arise by running simulations in which this was not the case, where a covariate significantly varies within segments. This would establish how badly the DSM performs when the segments are not sufficiently small and if distance sampling offers a viable alternative in this case. 

Leading on from this, when generating predictions outside the original study area, the estimates may be very different based upon the density prescribed at the edge. @Miller2013 highlights that at edges of the study area, both internal and external, models may link these areas and align the density between the two, without taking account for the area for which no animals may occur. They term this as 'smoothed across', implying that predictions could in theory generate unphysical abundance estimates, i.e. dolphins present on an island. When constructing the simulations, care was taken to ensure that the prediction area was cropped to the study region in an attempt to remove this effect. However, it is noted that a soap film smoother, a function of spatial location, could be used to account for these complex areas. Also mentioned within @Miller2013 is that even with smoothers, the density profile at the edge can still cause issues. There is the potential for smoothers to have covariates which cause the models fitted density surface to behave unrealistically when predicting the surface a substantial distance from the survey effort. @Miller2013 does note that this issue may be reduces by selecting an alternate smoother, with a generalisation of Duchon Splines suggested. A further area of study could focus on investigating these effects further, with the potential aim of constructing smoothers which are less prone to exhibiting this behaviour.

\pagebreak

# Conclusion

A number of interesting conclusions can be drawn from the various simulations conducted throughout this report. One of the aims of this report was to establish that DSM had be successfully implemented into the simulation set-up alongside the existing distance sampling method. From the results of all of the simulations above, there is little evidence to suggest that this was not successful, however, as noted within the discussion, further more demanding simulations should be conducted to verify this further.

Throughout the simulations conducted, the DSM approach has performed well in comparison to the existing distance sampling approach, in some cases better and in others worse. While there is some evidence that the abundance estimates for point transect designs may not be as accurate as one may wish, the simulation in which DSM performed significantly worse was the random parallel line design from the Buckland 2015 example case study. This study showed that the DSM approach severely underestimated the true variance of the estimates, a major concern, as well as its confidence interval coverage being below 90%.

Another result which was consistent throughout the simulations was the trend for DSM to overestimate the abundance in every single simulation. This was further tested at various points using wilcoxon tests, where the null was rejected in favour of the alternate that DSM estimates were greater than DS estimates. This does raise some concerns as overestimating the population is more problematic than underestimation.

Variance estimation appeared, in general, to be far better using DSM when line transects were used, with the variance estimates very similar using point transect designs. However it was noted that the majority of simulation used the default estimator 'R2' where random line placement is assumed, despite systematic placement being used. Both the stratification of the survey region, alongside using a more appropriate variance estimator were found to greatly improve the variance estimation for DS, making it comparable or in some cases better than that of DSM.

Overall, being able to simulate distance sampling designs and make design selections based on the results of both distance sampling analysis and density surface modelling provides a great benefit to researchers, taking advantage of the increased precision of the abundance estimates provided by DSM. Due to the likely increase in demand for spatial models, the inclusion of this at the survey design stage provides added assurances that should this be requested at a later stage, the design selected has been tested and the estimates produced are sufficiently accurate and precise for this analysis to take place.


\pagebreak
# References {-}
