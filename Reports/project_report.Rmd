---
title: "MMath Project:"
author: "Chris Martin"
date: "`r Sys.Date()`"
output: 
        bookdown::pdf_document2:
                fig_caption: yes
                toc: false

fontsize: 12pt
spacing: double
bibliography: references.bib
link-citations: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)

library(dsims)
library(knitr)
library(kableExtra)
library(dplyr)
library(moments)
```



Decleration:
_I certify that this project report has been written by me, is a record of work carried out by me, and is essentially different from work undertaken for any other purpose or assessment._

All of the code discussed and used throughout this report can be found at the following GitHub repository: https://github.com/Christopher-98/MMath-Project

\newpage
\tableofcontents




\pagebreak

# Abstract

\pagebreak 

# Introduction

One of the key aims in areas of applied ecological research is to determine the abundance of a particular population of interest, such as in a periodic way to monitor its development over time and determine changes, or to evaluate the potential effect of a new factor, such as a human disturbance. The size of the population can determine the importance of any new factors, with a smaller populations more under threat from a given factor compared to an abundant one @Buckland2015.

The aim of this report is to examine and investigate through simulations the extensions made to the _dsims_ R package through the inclusion of density surface modelling (DSM) simulations. The intended readership of this report are biologists or ecologists with statistical experience in distance sampling for estimating wildlife abundance. Further knowledge on distance sampling survey design would be beneficial to understanding some elements of the report in greater detail however this is not required.

Distance Sampling (@Buckland2001) is an extensive set of techniques used for estimating abundance within biological populations. These techniques have been widely used in both terrestrial ecology and aquatic studies to determine population size or density. The key idea of this approach is the assumption that not all animals are observed. Therefore, the probability of an animal being detected is estimated using the recorded distances at which observations are made. This allows for the total observations to be corrected to account for those animals missed and achieve an overall abundance estimate (@Buckland2015).

Density surface modelling builds upon the techniques of distance sampling by relating animal density to spatial variables within the environment, such as topology and habitat, while still correcting for animals not observed. The goal of this is to provide more information to wildlife managers to allow them to more appropriately manage the environment and the animals within @Hedley2004. This has lead to an increase in demand for spatial models, especially for analysing line transect data. These models, in addition to providing abundance estimates, allow for relationships between covariates and abundance to be investigated (@Miller2013).

The _dsims_ R package (@R-dsims) was created to allow researchers to randomise the construction of distance sampling designs to examine both the efficiency and bias of different designs with the goal of selecting the optimum survey design. This provides a great benefit to researchers, as failing to randomise the design selection can result in abundance estimates having considerable bias (@Buckland2015).

Currently, _dsims_ does not fit a density surface model during the course of its simulations, despite more precise abundance estimates, as identified by @Kat2007, alongside smaller variance estimates noted by @Miller2013 being available. Therefore, by including density surface modelling, improvements can be made to the estimates provided during these simulations. 

The more precise abundance estimates provided by density surface modelling, identified by @Kat2007, alongside smaller variance estimates noted by @Miller2013, are the key reasons for the inclusion of density surface modelling within simulation set-up. The inclusion therefore provides an added level of assurance to design selection decisions, as distance sampling simulations are used to determine the optimum survey design to estimate animal abundance for real world studies. For example, if a researcher wishes to undertake both density surface modelling and distance sampling analysis, the inclusion of both within the simulation at the design stage provides assurances that the optimum design for distance sampling analysis is also the optimum design for density surface modelling. Should there be no design which is optimum for both methods, the researcher can select which analysis type they wish prioritise and determine the optimum design for this.

Simulations will be designed and implemented to compare the accuracy and precision of abundance estimates generated through the design based approach, distance sampling, and the model based approach, density surface modelling. This is to ensure that the density surface modelling approach has successfully been implemented into the simulation set-up alongside the existing distance sampling approach.

\pagebreak

# Background research

## Plot Sampling

In order to determine a populations size, a census is one option available to those conducting the study. This involves counting every single individual within the population, similar to the UK completing a Census of its population every 10 years. However, in the natural world, this is only realistically possible in the simplest instances and therefore a different approach must be used (@Buckland2015). Researchers often use some form of sampling method to conduct a sample of the target population and draw conclusions for the overall population based on this sample.

The one of the most basic forms of sampling is plot sampling, with sampling designs being focused around either line transects or point transects. This comes alongside the assumption that every animal within each plot is observed.

Line transect sampling consists of a set of lines being placed over a study area by some predetermined method, for example systematic with regular spacing between each or randomly spaced, with both having a random starting point. The observer moves along each line, known as a transect, looking for animals or animal groups, referred to as clusters. These are defined by @Buckland2015 as "a group of animals with a well defined location for the group centre." For any animal or cluster the observer detects, they estimate or calculate the perpendicular distance _x_ of the animal or cluster from the nearest point of the line.

For point transect sampling, the transects are a set of points placed over the study area, with different placement methods available as with line transects. Most common is a systematically spaced grid with a random start point, as this provides the best coverage over the entire study region. At each point, the observer records any individuals or clusters observed from the point, along with the distance _r_ from the point at which the observation was made.

For this, the plots in line transect sampling are rectangles of dimension $2wl$, where _l_ is the length of a given transect and may change between transects depending on the survey design and area shape, and _w_ is the truncation distance. This is the distance from the line, beyond which observations are not recorded if the truncation distance is determined prior to the study. However, if the truncation distance is determined during the analysis phase, then this is the distance beyond which the observations are excluded from the analysis. For the plots in point transect sampling, these are circles of area $\pi w^2$ with the plot radius _w_ being the truncation distance.

These different designs are then used to calculate the animal density _D_ and therefore the overall abundance _N_ = _DA_, where _A_ represents the area of the study region. This is done by initially taking the total number of animals observed _n_ and dividing by the area of the plots _a_ to give $\hat{D} = n/a$. This then leads to the abundance estimate for the overall study area being $\hat{N} = \frac{nA}{a}$. For line transects, if we have k lines in total, then from above we have that $a = 2wL$ where $L = \sum_{i = 1}^k l_i$ is the the sum of the individual transect lengths $l_i$. This leads to the overall abundance estimate being:
$$ \hat{N} = \frac{nA}{2wL} $$
And, if we say have k point transects,from above $a = k\pi w^2$ leading to the abundance estimate of:
$$ \hat{N} = \frac{nA}{k\pi w^2} $$

## Distance Sampling

The one of most widely used methods of sampling for ecological populations is distance sampling, where information on the detectability of animals comes from the distances at which observations are made.

Distance sampling was first introduced by @Buckland1993 and includes a variety of designs such  as line and point transect sampling, which can then be used to estimate animal abundance using information on the distances to the individuals or clusters observed. The underpinning theory is that if the probability of animal detection can be estimated based on the sample observed, this can allow for estimates on how many animals were not observed and can therefore correct the abundance estimates to take this information into account. 

Distance sampling can therefore be thought of as a method of plot sampling, with the additional factor of not every animal on the plots being observed. The two simple designs of distance sampling are the same as those for plot sampling, with line transect sampling and point transect sampling designs available.




## Detection function {#detfunc}

To take into account the fact that not all animals or clusters within each transect are observed, the probability of detecting an animal with a transect must be estimated. This is done by using the distances to the animals observed, _x_ for line transects and _r_ for point transects, to fit a detection function $g(x)$, defined as 'the probability of detecting an animal that is distance _x_ $(0\le x \le w)$ from the line'. This can be similarly defined as $g(r)$ for point transects where _r_$(0\le r \le w)$ is the distance from the point. The normal technique is to assume that all animals on the point or line are definitely observed, such that $g(0) = 1$. Several detection functions can be defined by the user and a model selection criteria can be used to select the most appropriate for the data. However, all good detection function models share a set of properties, namely, they should have a shoulder, be non-increasing, be model robust, have pooling robustness and be efficient. A shoulder is when the probability of detection remains close to 1 as distance from the transect increases, before decreasing at a later point. Non-increasing suggests that the probability of detection at a far distance should not exceed the probability at any shorter distance from the transect. Model robustness is necessary as the true function is never know and as such, any models must be flexible to allow a range of different profiles to be modelled. Pooling robustness is a property whereby it is assumed that the model will not be affected if any covariates which influence detection are not included in the model, however these can be included through the use of multi-covariate detection functions, part of Multiple-covariate distance sampling (MCDS). Efficiency informs that should all other factors be equal, a preferable model is on which gives high precision, although high precision should not outweigh the need for the other properties to be satisfied.

Taking the detection function into account, we can then correct our abundance estimates from above to account for those animals not observed. This is done by defining $P_a$ as the expected proportion of animals observed within the study area. In the case of plot sampling above, $P_a = 1$ while for distance sampling $0 < P_a < 1$. $P_a$ can then be estimated based on the detection function $g(x)$ using an estimate $\hat{g}(x)$. This gives an estimate of $P_a$, $\hat{P_a}$ for line transects provided by the formula:
$$ \hat{P_a} = \frac{\int_0^w \hat{g}(x) dx}{w}$$
While for point transects:
$$ \hat{P_a} = \frac{2}{h(0)w^2}$$
where h(0) is the slope of the probability density function for detection distances _r_ , which is proportional to $g(r)r$, the detection function multiplied by _r_. This leads to abundance estimates from above becoming:
$$ \hat{N} = \frac{nA}{2wL\hat{P_a}} $$ for line transects and $$ \hat{N} = \frac{nA}{k\pi w^2 \hat{P_a}} $$ for point transects.

## Distance Sampling Simulations {#dsims}

All the material in this section is based on @Buckland2015 Prior to the simulation for a particular design being run, a number of objects must be first be defined. The first object is the study region, this can either be the default generated by R or user defined from a shapefile. Following this, a spatial distribution or density surface must be defined, from which animal locations can be generated based on the population description. The desired population size can be user defined and set for a series of simulations or be generated based on the spatial distribution supplied by the user. The desired truncation distance must then be defined and based on this an appropriate design can be generated.

The main considerations when constructing the design are the type, either line or point transects and the desired number or length of transects. If line transects are used, the design angle may be altered from its default of 0. A further parameter that must be set it whether plus or minus sampling should be used. Plus sampling is where the transects extend beyond the survey region into a buffer zone of distance _w_ from the edge, however only observations within the survey region are recorded. With minus sampling, transects end at the edge of the study region and do not extend beyond. Minus sampling will be used for all simulations throughout the remainder of this report. This is because in reality, most studies conduct minus sampling due to the additional cost associated with running a plus sampling survey, in addition to the fact that _dsims_ does not support plus sampling simulations at the present time. Based on the design, a set of survey transects can be generated, during which the detection process is simulated. The user can then define a detection function, based on either a half normal ('hn'), hazard rate ('hr') or uniform distribution ('uf') with a defined scale parameter and the desired truncation distance _w_, examples of which can be seen in Figure \@ref(fig:detect):
```{r detect,  fig.show = 'hold', out.width='50%', fig.cap='Examples of different detection functions: Left: Half Normal, Right: Hazard Rate'}
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/hn_detectfunc.jpg")))
plot_crop(include_graphics(paste0(getwd(),"/Reports/Plots/hr_detectfunc.jpg")))
```

Therefore for an animal at distance _x_ from the closest transect the probability of the animal being detected is given by the detection function evaluated at _x_, provided _x_ is less than or equal to _w_.

The distance data generated during a realisation of a survey is then analysed to estimate the abundance _N_ of the study area, with options available for several models to be analysed for each set of distance data, with a model selection criteria used to select the best, with AIC being the default.

At this stage, the encounter rate variance estimator can be specified, with a comprehensive summary of the various estimators found within @Fewster. Unfortunately, not all of the estimators mentioned within @Fewster are currently compatible with the survey realisations produced by _dsims_, with those compatible being 'R2','R3','O2', 'O3', 'P2' and 'P3'. The default estimator is 'R2' for line transects and 'P3' for point transects and unless otherwise stated, these were used within all simulations conducted in this report. @Fewster does however recommend the 'O2' estimator when using systematic parallel line transects, which will be investigated later in this report, however this estimator is only currently implemented for systematic parallel line designs.
If we look at how these estimators are constructed, it is possible to identify where one might be an improvement over the other. To begin assessing the variance, we revisit the density estimate as mention above, namely:

$$ \hat{D} = \frac{n}{\hat{P_a}} \times \frac{1}{2wL} =
\frac{1}{2w} \times \frac{n}{L} \times \frac{1}{\hat{P_a}} $$
Following on from this, the delta method is used to estimate the variance in the density, noting that $1/2w$ is a constant:

$$ [cv(\hat{D})]^2 = \bigg [ cv \bigg (\frac{n}{L}\bigg )\bigg]^2 + \bigg [ cv \bigg (\frac{1}{\hat{P_a}}\bigg )\bigg]^2$$

@Fewster notes that the encounter rate component of the variance, var(n/L) usually comprises around 75% of the total variance, while an estimate of var($\hat{P_a}$) can be obtained using standard likelihood theory. Variance estimators are required for n/L, as inference based upon a probability distribution is not likely to be sufficiently robust when dealing with inhomogeneous biological populations. This leads to estimators being used which are constructed around the empirical variability in each transects encounter rate, $n_i/l_i$. We first look at the construction of the 'R2' estimator. This is a design based estimator for random designs which arises as a result of a Taylor series expansion n/L about $\rho$, where  $\rho = E(\bar{n})/E(\bar{l})$ where $\bar{l}$ denotes L/k. This leads to the estimator becoming:
$$
\begin{aligned}
\widehat{var}_{R2}(\frac{n}{L}) &= \frac{k}{L^2}\times \frac{1}{k-1} \sum_{i = 1}^{k} (n_i - \hat{\rho}l_i)^2\\
        &=  \frac{k}{L^2(k-1)} \sum_{i = 1}^{k} l_i^2(\frac{n_i}{l_i} - \frac{n}{L} )^2\\
\end{aligned}
$$

which @Fewster notes is biased within the design framework. When using systematic designs, these often exhibit greater precision than those of random designs, leading to additional information which could be included within the variance estimate. However, this information is not taken advantage of if the estimator assumes a random line placement, as in 'R2', leading to the likely overestimation of the variance. To account for this increased precision, we can make use of the systematic nature of the design by grouping neighbouring transects in a post-stratification technique and then calculate the individual variances of each strata, before combining to give the overall estimate. To begin explaining this, we look at the 'S2' estimator, while noting this is not currently working within _dsims_. This if formed by splitting the transects into strata, where each transect can only be in one strata, with the strata labels being _h_ = 1, ..., H, and letting there be $k_h$ lines within stratum _h_. This implies:
$$ var(\frac{n}{L}) \approx \frac{1}{L^2} \sum_{h = 1}^{H} k_h \text{var}_h(n_{hj} - \rho l_{hj} )$$

where $n_{hj}$ and $l_{hj}$ are individual random variables $n_i$ and $l_i$ from within stratum _h_. The 'S2' estimator is then formed by combining the individual stratum estimates in an heuristic manner and adding a weighting factor to each, with this being the total line length for each stratum.

$$ \widehat{var}_{S2}(\frac{n}{L}) = \frac{1}{L^2} \sum_{h = 1}^{H} L_h^2 \widehat{var}_h(\frac{n_h}{L_h})$$ where $n_h$ and $L_h$ are the observation and line length totals for each strata, with $\widehat{var}_h$ being the within strata variance estimator.

As noted, this is not currently available within the _dsims_ package, however there is an analogous estimator which is available, 'O2'. This is constructed in a very similar way to the 'S2' estimator, however it makes uses of the systematic nature of the designs by having overlapping strata where the strata have a natural ordering, often by spatial proximity. In this case, the first strata contains lines 1 and 2, the second lines 2 and 3 and so on. This leads to a total of k-1 strata each with two lines in each and is described as an overlapping strata post-stratification scheme. The individual variances of the strata are then combined to give k-1 variance estimates, with @Fewster noting that while the degrees of freedom are not known, the approximation of k-1 is normally assumed. Due to the systematic construction of the strata, it is known at $k_h = 2$ and strata _h_ contains lines _h_ and _h+1_, leading to the new variance estimator becoming:

$$ \widehat{var}_{O2}(\frac{n}{L}) =  \frac{2k}{L^2(k-1)} \sum_{i = 1}^{k-1} \frac{(l_i l_{i+1})^2} {(l_i +l_{i+1})^2}\bigg (\frac{n_i}{l_i} - \frac{n_{i+1}}{l_{i+1}}\bigg )^2$$

Having selected the most appropriate or available variance estimator, all of the simulation elements are in place and the simulation can be run. This is done by generating a realisation of the survey based on the design, population and detection function specified before the analysis above is conducted, These operations are then repeated the specified number of times, say _R_, to obtain a set of simulations of animal distribution and survey design, alongside a corresponding set of estimates $\hat{N}$ of N. Typical values for _R_ are between 100 and 1000. The estimates along with other information are then extracted from the survey data allowing mean estimates and confidence intervals alongside other statistics to be calculated. 

\pagebreak

## Density Surface Modelling

Density surface modelling extends the technique of distance sampling by relating animal density to spatial covariates within the environment, such as topology or habitat. The goal of this is to provide more information to allow for more appropriate management of the environment and the animals within (@Hedley2004). This has lead to an increase in demand for spatial models, especially for analysing line transect data. These models, in addition to providing abundance estimates, allow for relationships between covariates and abundance to be investigated (@Miller2013). The advantage of using this method over distance sampling is reduced variance in abundance estimates. This is due to the model being able to explain the inter-transect variation, a large component of the distance sampling variance (@Miller2013). However, a disadvantage of this method is the risk of bias being introduced into the abundance estimates through improperly specifying the model (@Hedley2004).

Density surface modelling can be implemented using either a one or two stage approach. Using a two stage approach, the detection function is fitted first then subsequently fitting a spatial model, while the one stage approach leads to estimating the detection and spatial parameters simultaneously. @Miller2013 states that 'Generally, very little information is lost by taking the two stage approach' as transect width is comparably smaller than that of the study region. Therefore, provided the density does not differ appreciably across the width of the transect or within the point, no information is lost by the two stage approach, as detection distances don't provide information of the spatial distribution (@Miller2013). This may lead is issues occurring where the density of the species has significant variability at the transect level. However, one drawback of the two stage model is that, to accurately evaluate the model uncertainty, the uncertainty in both the detection function and the spatial models should be suitably combined. The delta method, as mentioned in Section \@ref(dsims), is currently implemented within the software to propagate the variance in the detection function and the spatial model into the variance of the abundance estimates (@Bravington2021). The remainder of this report refers to methods implemented using only the two stage approach.

Initially, the detection function must be fitted, with the specification being the same as mentioned in Section \@ref(detfunc) above. Following this, the density surface model can be fitted. To enable this to occur, the transect data must be separated into segments. This is easily done for point transects with each point being a segment however it more complicated for line transects.

With line transects, they must be split up into J segments of length $l_j$. It is normal from the segments to be approximately square, with dimensions of $2w \times 2w$ where _w_ is the truncation distance of the design (@Miller2013). From here, the segment areas enter the model as part of an offset, to allow for non-constant segment areas. This leads to the line transect segments having an area of 2$wl_j$ and the point transect segments an area of $\pi w^2$. In the model, the counts or abundances are using a generalised additive model (GAM) using the sum of the smoothed covariates. 

### Response models

The model used when the count per segment is used as the response is:

$$ \mathbb{E}(n_j) = \hat{p_j}A_jexp[ \, \beta_0 + \sum_k f_k(z_{jk} ) ]\,$$

Where $f_k$ are the smoothed functions of the covariates and $\beta_0$ is the intercept term. By multiplying the segment area $A_j$ by the estimated probability of detection $p_j$ this gives the effective area of the segment, acting as an offset to account for different segment areas. Where distance is the only covariate in the detection function, $p_j$ is constant across all segments and therefore $\hat{p_j} = \hat{p} \forall j$. The distribution of $n_j$ can then be modeled using an overdispersed Poisson, Negative binomial or Tweedie distribution. 

An alternative to using this is to use abundance estimates for each segment generated by distance sampling as the response. To do this, the response $n_j$ is replaced by an estimator of the abundance in each section, $\hat{N_j}$ where this is defined as: 
$$ \hat{N_j} = \sum_{r = 1}^{R_j}  \frac{s_{jr}}{\hat{p_j}}$$

Where $R_j$ is the number of observations in the jth segment and $s_{jr}$ is the size of the rth group observed, with this being 1 if only individuals are observed. As identified by @Buckland2015, this is an Horvitzâ€“Thompson-like estimator of the segment abundance, allowing for covariates to be included through $\hat{p_j}$. The fitted model then becomes:
$$ \mathbb{E}(\hat{N_j}) = A_jexp [ \, \beta_0 + \sum_k f_k(z_{jk} ) ]\, $$
Where the model follows the same three distributions as before. The main difference between these models is that the offset is now the physical area of each segment, as opposed to the effective area in the first model for $n_j$. The model selected to be implemented within the simulation was the count per segment model, as modelling actual counts is preferable to modelling estimates for abundance (@Miller2013).

To allow for a DSM to predict abundance, a series of prediction cells must be defined. These are not necessarily restricted to just the original study region, allowing for regions outside the study area to be predicted over. Each of the prediction cells must include the same covariates as specified in the DSM, including the area of each cell. Predictions can then be made for the abundance in each cell and by summing these over the whole region, an overall abundance estimate can be obtained. The size of the prediction cells may be specified by the user, however cells 'smaller than the resolution of the spatially referenced data' do not have an influence on the abundance estimates produced by DSM.

\pagebreak

# Simulation extentions

This section details the extensions I made to the dsims package Marshall (2021) as part of this project.

## Prediction Grid

The prediction grid, as required by the DSM, is constructed across the study region prior to the simulation beginning, with the resolution of the grid set at half the truncation distance of the design. As @Miller2013 noted, smaller cells sizes could be used but there is a limit since using cells smaller than the spatial data resolution will not have an effect on the abundance estimates provided by DSM, and there is also the computational increase resulting from smaller cell sizes. This can be specified prior to the simulation as it is only dependant on the study region and will not change between iterations of the simulation.

Existing code to construct this prediction grid can be found in @Souchay20 and this was used in early testing and simulations. However, it was noted that some of the R packages used in this code are due to be retired at the end of 2023, namely the _rgeos_ package (@R-rgeos). Therefore, the decision was made to rewrite this code using generic functions from the far more widely used Simple Feature (sf) package (@R-sf). The prediction grid is currently set-up to create the grid over the entire study region, to allow for a comparison with the distance sampling approach. However, this is not a restriction and any prediction grid could be specified, including areas outside of the study region. This would allow for simulations to predict the abundance on areas not specifically studied in the survey, which the distance sampling approach cannot do without increasing the size of the study region itself. This offers a distinct advantage for DSM over the distance sampling approach, however it may be possible to generate unrealistic abundance estimates through extrapolations outside the range of data used in the model (@Miller2013). 

## Segment Data

For each iteration of the simulation, the observation data and segment data must be extracted from the survey realisation. To do this, a function _generate.dsm.data_ was written which first extracts the observation data from the survey and subsequently splits into the segments required by the DSM approach, using a function _to_segments_ which was written for this explicit purpose.

For Point transect designs, each point is treated as its own segment. However, in the case of line transect designs, the lines are split to allow them to be modelled as points. Each line is split into segments of approximate length $2w$ with _w_ being the truncation distance, as suggested by @Miller2013 and each segment assigned its own unique sample label. Subsequently, the length of each segment is recorded as the segments effort value and the centre of the segment as its location. Polygons of each segment are then created using _st_buffer_, with _w_ being used as the distance. This leads to squares of approximately $2w \times 2w$ for line transects and circles of radius _w_ for points.

These allow the area of each segment to be calculated, a requirement for the DSM model. This is calculated using _st_area_ for both the squares and circles from the respective transect types, on the intersection between the polygons and the outer boundary of the study region. This ensures only areas within the study region are counted towards segment area and internal strata boundary's do not split segments. Failure to do this results in the areas of some segments being larger than they are in the survey, and as a result the DSM abundance estimate is smaller, since the prediction grid is only over the survey area.

Once the segment areas have been calculated, the segments can be linked to the observation data by allocating each observation to the nearest segment and giving this the respective segments sample label in the observation data, overwriting the original allocation. Additionally, the coordinate reference system of the observation data is set as that of the segments to ensure consistency when using different reference systems to the default. 

## Modeling

Based on this data, both a distance sampling model and density surface model are constructed, with the DSM modeling the counts against the smooth of spatial locations using a tweedie error distribution. This error distribution is used as it offers a greater degree of flexibility than the dispersed-poisson or negative binomial (@Miller2013). The formulas for both could be changed to include environmental or other covariates such as strata, allowing for different detection functions to be fit to each strata.

In the smoothed term, the degrees of freedom is restricted to the total number of transects, as without doing this, the simulation was prone to failing to fit a model. The limit for this is the number of segments in the model, which in the case of point transects is the total number of transects. While this could be far larger for line transects, this would increase the computational requirements beyond a reasonable level and without offering a significant improvement, added to the fact that this model will be run within a simulation which is already computationally intensive. The abundance estimates are extracted from both models and stored alongside other statistics from each including the standard errors and confidence intervals for each model.

All elements of the distance sampling simulation were kept as is, with the additions made to facilitate the implementation of DSM designed to ensure the generality of the code for potential expansion in future.

\pagebreak

# Simulation - Default Region

The simulation was initially tested using the default region generated by _dsims_ @R-dsims with a truncation distance of 60. Both point and line transect designs were run with an aim of 25 and 12 samplers for the respective designs. A basic test density was then constructed for the region with high and low spots as seen below in Figure \@ref(fig:defregion) with relatively gently gradients. 

``` {r defregion, crop = TRUE, fig.cap='Density surface used for the Default region'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default_density.jpg')))
```

A population description was then constructed based on this density surface with a true population of 1000. A detection function was then defined as a half normal with scale parameter of 30 for both designs, producing the detection function seen in Figure \@ref(fig:defdetect)

```{r defdetect, out.width='50%', fig.align='center', fig.cap='Detection function for the default region, a half normal with scale parameter of 30'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default_detect.jpg')))
```

An example survey for each of the designs is displayed below in Figure \@ref(fig:defsurvey).
```{r defsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example Surveys for the default region. Top: Point Transect design, Middle: Parallel line design, Bottom: Zigzag line design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Default survey zigzag.jpg')))
```


## Results

Having completed 5000 simulations for both the distance sampling and density surface models with each design, we can now examine and compare these to give us an insight into the circumstances under which a particular model is better or worse than the other.

### Default Region Point design

For the initial default region with the point transect design, the histograms of both the distance sampling and DSM abundance estimates are displayed below in Figure \@ref(fig:histdefpoint):
```{r histdefpoint, fig.show = 'hold', out.width='50%' , fig.cap='Histograms of Point Design estimates for Default region'}
default.point <- read.csv('Estimates/Default1000point.csv')

hist(default.point$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(default.point$dsm.est), col = 'red')

hist(default.point$ds.est,breaks = 50,
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(default.point$ds.est), col = 'red')
```

These plots show that the estimates for both the DSM and DS methods appeared fairly symmetrical around the true abundance, with a slight right skew present. If we now compare the means of the two approaches, the mean of DSM estimates was `r round(mean(default.point$dsm.est),2)` and the mean of DS estimates was `r round(mean(default.point$ds.est),2)`. It can be seen that the mean of the DSM estimates appears a good distance from the true population of 1000 while the distance sampling method is closer.

```{r resdefpoint}
results.point <- read.csv('Results/results default 1000 point.csv')
results.point$X <- NULL

results.point <- results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage')) %>%
        `colnames<-`(c('DSM results', 'DS results'))

kable(results.point,
      caption = 'Default region results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

Table \@ref(tab:resdefpoint) above provides statistics which allow us to compare the variability of the models as well as their ability to capture truth within their confidence intervals. We see in this case that the DSM method has a larger and positive bias compared to the smaller and negative bis of the DS method, indicating the DS method produced more accurate results in this case. This trend continues with the Mean Standard errors and standard deviation of the estimates being similar but again the DS method producing a smaller error. It is noted here that both approach underestimate the true variance in this case, however DS is overall a closer estimate. The DSM does produce a smaller Coefficient of variation, however the difference is marginal. Next, we can evaluate how many times the true abundance was within the 95% CI for every model computed. The results show that the DS approach performed better at `r results.point[6,2]` compared with `r results.point[6,1]` for the DSM method. However, both of these are below the 95% level which would raise some concerns regarding the suitability of the design. 

### Default region parallel Line design

Now examining the results of the line transect design, on the same density surface, we see the histograms of the two estimates below in Figure \@ref(fig:histdefline):

```{r histdefline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Line Design estimates for Default region'}
default.line <-  read.csv('Estimates/Default1000line.csv')

hist(default.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(default.line$dsm.est), col = 'red')

hist(default.line$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(default.line$ds.est), col = 'red')
```

These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth. Investigating this further, we find the mean of DSM estimates to be `r round(mean(default.line$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(default.line$ds.est),2)` from Table \@ref(tab:resdefline). These estimates are both very close to the true value, possibly as a result a large portion of the survey area being covered by the transects.
``` {r resdefline}
results.line <- read.csv('Results/results default 1000 line.csv')

results.line$X <- NULL

results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'Default region results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From table \@ref(tab:resdefline) above, it can be observed that in this case, the DSM method overall performed better than the DS method, having a slightly smaller bias as well as a dramatically smaller mean standard error and mean coefficient of variation. However, for the DS method, the standard deviation of the estimates is markedly less than the mean standard error, indicating this method provides better precision than the standard errors suggest. This is also the case with the DSM method however the difference is far smaller. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`.

### Default region Zigzag Line design

Now examining the results of the zigzag line transect design, on the same density surface, we see the histograms of the two estimates below in Figure \@ref(fig:histdefzig) :

```{r histdefzig, fig.show = 'hold', out.width='50%' , fig.cap='Histograms of Zigzag Design estimates for Default region'}
default.zig <-  read.csv('Estimates/Default1000linezigzag.csv')

hist(default.zig$dsm.est,breaks = 50, main = 'Histogram of DSM estimates')
abline(v = mean(default.zig$dsm.est), col = 'red')
hist(default.zig$ds.est,breaks = 50, main = 'Histogram of DS estimates')
abline(v = mean(default.zig$ds.est), col = 'red')
```

These plots differ slightly to their point and parallel line counterparts, with the mean appearing further away from truth than previously. Investigating this further, we find the means of DSM estimates to be `r round(mean(default.zig$dsm.est),2)` and the mean of DS estimates to be `r round(mean(default.zig$ds.est), 2)`. These estimates are both very close to the true value, possibly as a result a large portion of the survey area being covered by the transects.

```{r resdefzig}
results.zig <- read.csv('Results/results default 1000 zigzag.csv')
results.zig$X <- NULL

results.zig <- results.zig %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zig,
      caption = 'Default region results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Zigzag Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "HOLD_position")
```

From Table \@ref(tab:resdefzig), the DSM model performs relatively well, having a reasonably small bias. The mean standard error is in fact lower than that of the DS model, indicating higher precision on average across the models, with the standard deviation of the estimates being lower in both cases, suggesting greater precision for both modes than the standard errors imply. The mean coefficient of variation is slightly lower for the DSM model as a direct result of the reduced standard errors. However the confidence interval coverage is higher with the DS model, while this is due to the larger variance estimates widening the confidence interval, indicating truth has a higher probability of being captured within the DS models confidence interval. 

\pagebreak

# Simulation - North Sea {#NS}

Having gone through the process and results for the default region produced by _dsims_, a more complex example was created to further test the capabilities of the simulations with a real world example. This region with be in the area of the North sea off the east coast of the UK. As this would likely be a shipborne or airborne study, point transects are not realistic in this scenario, however they will be included for completeness. To assess the performance of both the distance sampling and density surface modelling approaches, point and line transect designs will be examined. These will be two point transect designs and four line transect designs, with one of each type using a stratified design with the region split in two. The standard designs will be compared first before examining the stratified designs. All designs used a truncation distance of 10 with the number of transects varying between the point and line transect designs. This difference is ensure the covered area of each study is similar to allow the results to be compared, as well as ensure sufficient observations to fit the detection function can be obtained. The density maps for stratified and non-stratified will be different to justify the use of different strata and examine the models ability to pick this up. These density surfaces can be seen in Figure \@ref(fig:NSden) below.
```{r NSden, crop = TRUE, fig.show='hold', out.width='50%', fig.cap='Density surfaces for the basic and stratified North Sea regions'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North_Sea_density.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North_Sea_Strata_density.jpg')))

```
As before, the detection function was kept identical between each design for both stratified and non-stratified. In this case it was a Half Normal detection function with a scale parameter of 5, which can be seen in Figure \@ref(fig:NSdet) below:
```{r NSdet, out.width='75%', fig.align='center', , fig.cap='Plot of the detection function used in both the basic and stratified North Sea regions: A half Normal with scale parameter 5 and truncation 10'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Detect.jpg')))
```
## Non-Stratified Designs

The point transect designs aimed for 70 samplers and the line transects for 25 samplers. The standard line designs consisted of a systematic parallel line design with transects perpendicular to the coast and a zigzag line design originating in the North of the region. An example of each design can be seen in Figure \@ref(fig:NSsurvey) below:
```{r NSsurvey, fig.show = 'hold', out.width='45%', out.height='75%', fig.cap='Examples of surveys for the North Sea region. Top Left: Point Transect design, Top Right: Parallel line design, Bottom Left: Zigzag line design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea survey zigzag.jpg')))
```

The simulation was run 5000 times with the DSM model using a tweedie error distribution and the only covariates being spatial coordinates.

\pagebreak

### Non Stratifed Results - Point Design

For the point transect design, the histograms of the abundance estimates from each simulation run are displayed below in Figure \@ref(fig:histNSpoint):
```{r histNSpoint, fig.show = 'hold', out.width='50%', fig.cap='Histograms of the abundance estimates for the North Sea region using a point transect design'}
NS.point <- read.csv('Estimates/North Sea5000point.csv')

hist(NS.point$dsm.est,
     breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.point$dsm.est), col = 'red')

hist(NS.point$ds.est,
     breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.point$ds.est), col = 'red')
```

These plots suggest both methods are relatively good at estimating the true abundance of 1000, with both methods appearing to overestimate, with the DSM method overestimating slightly more than the DS method. There also appears to be a slight right skewness to both plots which will be looked at analytically. Investigating the means further, we find the mean of DSM estimates to be `r round(mean(NS.point$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(NS.point$ds.est),2)`. These estimates are both close to the true abundance, with the overestimation present as mentioned above. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NS.point$dsm.est - NS.point$ds.est) > 0) / nrow(NS.point) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.point$dsm.est, NS.point$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. If we now compare the other variables extracted from the models in the below, we can examine the accuracy and variability of each method:
``` {r resNSpt}
results.point <- read.csv('Results/results NS 5000 point.csv')

results.point$X <- NULL

results.point <-  results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.point,
      caption = 'North Sea Non-Stratified results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From the results in Table \@ref(tab:resNSpt), it can be observed that in this case, the DS method overall performed better than the DSM method, having a smaller bias as well as higher confidence interval converge, despite having a smaller Mean Std error. Additionally, the standard deviation of the estimates is slightly less for the DS method, however both methods do a relatively good job at estimating this. Additionally, we observe from the skewness statistics that both methods returned a positive statistic indicating the right-skewed as identified above, which suggests the right tail is longer and this is supported by some of the estimates being over 2000, with the maximums for DSM and DS being `r round(max(NS.point$dsm.est),2) ` and `r round(max(NS.point$ds.est),2)` respectively. The kurtosis statistics for both methods are greater than 3, indicating we would expect more frequent outliers than a normal distribution. While the Mean CV for the DSM method is lower, the difference is marginal and does not out-way the bias or variance issues of this method. Overall this clearly seems to be a case where the DS method performs better than the DSM estimation method.

### Non Stratifed Results - Parallel Line Design

Now examining the results of the line transect design, on the same density surface, we see the histograms of the two estimates below in Figure \@ref(fig:histNSline):

```{r histNSline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Parallel Design estimates for North Sea Region'}
NS.line <-  read.csv('Estimates/North Sea5000line.csv')

hist(NS.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.line$dsm.est), col = 'red')

hist(NS.line$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.line$ds.est), col = 'red')
```

These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth. However, there appears to be less skewness for these than is present with the point transect results. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NS.line$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NS.line$ds.est), 2)`, as highlighted in Table \@ref(tab:resNSline). Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NS.line$dsm.est - NS.line$ds.est) > 0) / nrow(NS.line) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.line$dsm.est, NS.line$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.
``` {r resNSline}
results.line <- read.csv('Results/results NS 5000 line.csv')

results.line$X <- NULL

results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'North Sea Non-Stratified results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From the results in Table \@ref(tab:resNSline), it can be seen that in this case, the performance of both models was similar and each performed well in different areas compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better error estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`, however this is directly due to the higher Std errors from the DS method widening the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the kurtosis is very similar to that of a normal distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NS.line$dsm.est)$p.value` and `r jarque.test(NS.line$ds.est)$p.value` respectively. These indicate we can not reject the null hypothesis that these estimates are normally distributed. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 

### Non Stratifed Results - Zigzag Line Design

Now examining the results of the zigzag line design, on the same density surface, we see the histograms of the two estimates below:

```{r histNSzig, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Zigzag Design estimates for North Sea Region'}
NS.zig <-  read.csv('Estimates/North Sea5000linezigzag.csv')

hist(NS.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NS.zig$dsm.est), col = 'red')

hist(NS.zig$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NS.zig$ds.est), col = 'red')
```

These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth. However, there appears to be less skewness for these than is present with the point transect results. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NS.zig$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NS.zig$ds.est), 2)`, as highlighted in Table \@ref(tab:resNSzig). Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NS.zig$dsm.est - NS.zig$ds.est) > 0) / nrow(NS.zig) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.zig$dsm.est, NS.zig$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.
``` {r resNSzig}
results.zigzag <- read.csv('Results/results NS 5000 zigzag.csv')

results.zigzag$X <- NULL

results.zigzag <-  results.zigzag %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))

kable(results.zigzag,
      caption = 'North Sea Non-Stratified results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From the results in Table \@ref(tab:resNSzig), it can be seen that in this case, the performance of both models was similar and each performed well in different statistics compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better error estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.zigzag[6,2]` compared to `r results.zigzag[6,1]`, however this difference is likely due to the higher Std errors from the DS method wideing the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the kurtosis is very similar to that of a normal distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NS.zig$dsm.est)$p.value` and `r jarque.test(NS.zig$ds.est)$p.value` respectively. These indicate we can not reject the null hypothesis that these estimates are normally distributed. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 

## Stratified Designs

For the stratified designs, it was decided to split the region into two strata, a large northern strata 'North' and a smaller southern strata 'South' to test how the simulation and both methods cope with a stratified design, alongside ensuring th . The point transect designs aimed for 60 samplers, with 30 in each strata, and the line transects for 25 samplers with 10 in the North strata and 15 in the South strata. The standard line designs consisted of a systematic parallel line design with transects perpendicular to the coast for both the North and South Stratum. The zigzag line design selected has the transects in the North strata originating in the North of the Strata, with a design angle of 90$^\circ$, while in the South Strata, transects originate on the Western boundary with a design angle of 0$^\circ$. An example of each design, including the point design, can be seen in Figure \@ref(fig:NSSsurvey) below:
```{r NSSsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example surveys for each Stratified Designs. Top Left: Point Transect design, Top Right: Parallel line design, Bottom Left: Zigzag line design'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey point.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/North Sea Strata survey zigzag.jpg')))
```

The simulation was run 5000 times with the DSM model using a tweedie error distribution and the only covariates being spatial coordinates.

\pagebreak
### Stratifed Results - Point Design

For the point transect design, the histograms of the abundance estimates from each simulation run are displayed below in Figure \@ref(fig:histNSSpoint):
```{r histNSSpoint, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Point Design abundance estimates for Stratified North Sea Region'}
NSS.point <- read.csv('Estimates/North Sea Strat5000point.csv')

hist(NSS.point$dsm.est,
     breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.point$dsm.est), col = 'red')

hist(NSS.point$ds.est,
     breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.point$ds.est), col = 'red')
```

These plots suggest both methods are relatively good at estimating the true abundance of 1000, with both methods appearing to overestimate, with the DSM method overestimating slightly more than the DS method. There again appears to be a slight right skewness to both plots which will be looked at analytically below. Investigating the means further, we find the mean of DSM estimates to be `r round(mean(NSS.point$dsm.est),2) ` and the mean of DS estimates to be `r round(mean(NSS.point$ds.est),2)`. These estimates are both close to the true abundance, with the overestimation present as mentioned above and to a very similar level to the Non-Stratified point results. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.point$dsm.est - NSS.point$ds.est) > 0) / nrow(NSS.point) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NSS.point$dsm.est, NSS.point$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the difference between the DSM and DS estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. If we now compare the other variables extracted from the models below, we can examine the accuracy and variability of each method:
``` {r resNSSpoint}
results.point <- read.csv('Results/results NS Strat 5000 point.csv')

results.point$X <- NULL

results.point <-  results.point %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.point,
      caption = 'North Sea Stratified results - Point') %>%
        add_header_above(c('Statistic' = 1, 'Point Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From Table \@ref(tab:resNSSpoint), it can be observed that in this case, the DS method overall performed better than the DSM method, having a smaller bias as well as higher confidence interval coverage, despite having a smaller Mean Std error. Additionally, the standard deviation of the estimates is slightly less for the DS method, however both methods do a relatively good job at estimating this. Additionally, we observe from the skewness statistics that both methods returned a positive statistic indicating the right-skewed as identified above, which suggests the right tail is longer and this is supported by some of the estimates being close to 2000, with the maximums for DSM and DS being `r round(max(NSS.point$dsm.est),2) ` and `r round(max(NSS.point$ds.est),2)` respectively. The kurtosis figures for both methods are greater than 3, indicating we would expect more frequent outliers than a normal distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NSS.point$dsm.est)$p.value` and `r jarque.test(NSS.point$ds.est)$p.value` respectively. These indicate we can reject the null hypothesis that these estimates are normally distributed, therefore our use of the Wilcoxon test above is justified.    While the Mean CV for the DSM method is lower, the difference is marginal and does not out-way the bias or variance issues of this method. Overall this clearly seems to be a case where the DS method performs better than the DSM estimation method.

### Stratifed Results - Parallel Line Design

Now examining the results of the line transect design, on the same density surface, we see the histograms of the estimates below in Figure \@ref(fig:histNSSline):

```{r histNSSline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Parallel Design estimates for North Sea Region' }
NSS.line <-  read.csv('Estimates/North Sea Strat5000line.csv')

hist(NSS.line$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.line$dsm.est), col = 'red')

hist(NSS.line$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.line$ds.est), col = 'red')
```

These plots are similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth. However, there appears to be less skewness for these than is present with the point transect results, exhibiting a slightly sharper point at the mean. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NSS.line$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NSS.line$ds.est), 2)`. These estimates are very close to the truth, with DSM again overestimating in comparison to DS. Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.line$dsm.est - NSS.line$ds.est) > 0) / nrow(NSS.line) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS. `r test <- wilcox.test(NSS.line$dsm.est, NSS.line$ds.est, paired = T, alternative = 'greater')` This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the difference between the DSM and DS estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.
``` {r resNSSline}
results.line <- read.csv('Results/results NS Strat 5000 line.csv')

results.line$X <- NULL

results.line <-  results.line %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.line,
      caption = 'North Sea Stratified results - Line') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From Table \@ref(tab:resNSSline) above, it can be seen that in this case, the performance of both models was very similar throuhgout all areas, with the exception being the mean estimates and bias. The mean estimate for DSM was again a slight overestimation and as a result, bias was positive, while DS underestimated by almost the same amount, leading to an almost identical but negative bias. The Mean Std errors and Std Deviation of estimates were slightly lower for DSM, however the difference in both cases is very small. The Mean CV for each method is very similar, being slightly in favour of the DSM method although the difference is negligible. The DS method has a slightly higher probability of truth being contained within its confidence intervals, at `r results.line[6,2]` compared to `r results.line[6,1]`, however this small difference is likely due to the slightly higher Std errors from the DS method widening the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is small. The kurtosis scores are close to 3 but larger, suggesting the distribution of the estimates are is similar to that of a normal distribution, with a slight trend towards a leptokurtic distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NSS.line$dsm.est)$p.value` and `r jarque.test(NSS.line$ds.est)$p.value` respectively. These indicate we can reject the null hypothesis that these estimates are normally distributed and our use of the Wilcoxon test above is justified, since the normality assumption would have been violated for a parametric test. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation. 

### Stratifed Results - Zigzag Line Design

Unfortunately, for the stratified zigzag design, an error intermittently occurred within the simulation which caused the fitting of the density surface model to fail. Upon further inspection, this error did not occur within any of the code written as part of this project. After consultation with L Marshall, the author of the _dsims_ package, it was determined the error likely occurred as a result of how some of the survey realisations within _dsims_ were constructed. This likely led to issues with the coordinates for the transects and segments, with this not being an issue within the DS model however the density surface model would not accept these, resulting in the error. It is outside the scope of this project to investigate the root cause of this error, therefore L Marshall was notified of the bug and given access to the code to determine the root cause. As mentioned, this error occurred intermittently, resulting in a full set of 5000 iterations not being completed in a single run. However, multiple runs were completed with the exact same conditions until a total of 5000 were completed, with these results being examined below. Examining these results, we see the histograms of the two abundance estimates below in Figure \@ref(fig:histNSSzig):

```{r histNSSzig, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Abundance estimates generated by a zigzag line design for the Stratified North Sea region.' }
NSS.zig <-  read.csv('Estimates/North Sea Strat5000linezigzag.csv')

hist(NSS.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(NSS.zig$dsm.est), col = 'red')

hist(NSS.zig$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(NSS.zig$ds.est), col = 'red')
```

These plots are very similar to their point transect counterparts, with the mean for each signified by the red line appearing very close to truth.However, there appears to be less skewness for these than is present with the point transect results. Investigating this further, we find the mean of DSM estimates to be `r round(mean(NSS.zig$dsm.est), 2) ` and the mean of DS estimates to be `r round(mean(NSS.zig$ds.est), 2)`. These estimates are very close to the true Overall, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((NSS.zig$dsm.est - NSS.zig$ds.est) > 0) / nrow(NSS.zig) *100` % of the time, indicating it is highly likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(NS.zig$dsm.est, NS.zig$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS. Examining the other statistics extracted from the simulation.
``` {r resNSSzig}
results.zigzag <- read.csv('Results/results NS Strat 5000 zigzag.csv')

results.zigzag$X <- NULL

results.zigzag <-  results.zigzag %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zigzag,
      caption = 'North Sea Non-Stratified results - Zigzag') %>%
        add_header_above(c('Statistic' = 1, 'Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```
From the results in Table \@ref(tab:resNSSzig) above, it can be seen that in this case, the performance of both models was similar and each performed well in different areas compared to the each other. The mean estimate was again a slight overestimation and as a result, bias was slightly higher for DSM, however this difference was comparatively small. The Mean Std errors were lower for DSM and far closer to the Std Dev of the estimates, highlighting the better error estimation of DSM. However the Std Dev of the estimates was slightly smaller for the DS approach. The DS method also has a higher probability of truth being contained within its confidence intervals, at `r results.zigzag[6,2]` compared to `r results.zigzag[6,1]`, however this difference is likely due to the higher Std errors from the DS method wideing the confidence interval. Despite this, both methods achieved truth in their confidence intervals over the 95% significance level. The skewness scores for both are very similar and suggest again a slight right skew however this is very small. The kurtosis scores are very close to 3, suggesting the kurtosis is very similar to that of a normal distribution. In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(NSS.zig$dsm.est)$p.value` and `r jarque.test(NSS.zig$ds.est)$p.value` respectively. These indicate we can not reject the null hypothesis that these estimates are normally distributed. Overall, while the mean estimate appears in favour of the DS method, the DSM approach can be said to have performed better in this simulation, due to the small bias, lower Mean Std error and coefficients of variation.

## Discussion of Results

\pagebreak

# Simulation - Buckland 2015 Example

For our next set of simulations to examine the capabilities of the DSM approach in respect to the DS method, I will revisit the the example simulation provided in Section 2.5.2 of @Buckland2015 and use both a DS and DSM approach to repeat these simulations. It must be noted that the original analysis was part of a survey design case study by Laura Marshall and used the R package _DSsim_ @R-DSsim . For the purpose of this study, each of the designs and simulations will be translated into the _dsims_ @R-dsims package used throughout the remainder of the simulations to ensure comparability with the DSM approach developed. All details from the original designs will be kept constant, with the population being fixed at 1500 individuals and using a truncation distance of 1000m. For each of the surveys, a half-normal detection function with a scale parameter of 500 will be assumed. The density surfaced used is displayed below alongside the detection function in Figure \@ref(fig:Montdensdetect).
```{r Montdensdetect, fig.show = 'hold', out.width='50%', fig.cap='Density surface and detection function for Buckland 2015 example'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave density.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave detect.jpg')))
```


By using this study region, it is possible to verify the simulation method developed for DSM can be run on study regions with internal holes, such as islands on a seaborne survey. The three different designs will be used, being the same as those tested in the case study in @Buckland2015. These will be a systematic parallel line design, a random parallel line design and a zigzag design. Examples of each design are displayed in Figure \@ref(fig:Montsurvey) below:

```{r Montsurvey, fig.show = 'hold', out.height='33%', fig.cap='Example surveys for each Design: Top Left: Random Parallel Line, Top Right: Systematic Parallell line, Bottom Left: Zigzag line'}
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey random line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey parallel line.jpg')))
plot_crop(include_graphics(paste0(getwd(),'/Reports/Plots/Montrave survey zigzag.jpg')))
```
As with each of the simulations run as part of the North Sea section, 5000 iterations will be run for each design. While this is different from the 100 iterations originally conducted, the increase in the number of iterations will increase the reliability of the results.

## Results - Random Parallel Line

We can now examine the results of the Random parallel line design by initially plotting histograms of the estimates from both the DS and DSM methods below in Figure \@ref(fig:histMontrand):

```{r histMontrand, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Random Parallel Line Design estimates for Buckland 2015 example region'}
Mont.rand <-  read.csv('Estimates/Montrave5000linerandom.csv')

hist(Mont.rand$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(Mont.rand$dsm.est), col = 'red')

hist(Mont.rand$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(Mont.rand$ds.est), col = 'red')
```

While the plot for the DS estimates appears similar to those we have seen before, the estimates for the DSM method appear markedly different. This is due to the prescence of very extreme overestimates with the largest estimate being `r round(max(Mont.rand$dsm.est))`, over 3 times the true population. In addition to this, `r sum(Mont.rand$dsm.est > max(Mont.rand$ds.est))` of the DSM estimates were greater than the largest DS estimate of `r round(max(Mont.rand$ds.est))`  It initially appears that the majority of the DSM estimates are centred around the true population of 1500, however this will be investigated further below. As with the previous simulations, it was found that the estimates produced by DSM were greater than those produced by DS from the same survey data `r sum((Mont.rand$dsm.est - Mont.rand$ds.est) > 0) / nrow(Mont.rand) *100` % of the time, indicating it is likely DSM will produce an estimate greater than that of DS. A Wilcoxon test was performed on both sets of estimates with the alternative being that DSM was greater than DS.
``` {r}
test <- wilcox.test(Mont.rand$dsm.est, Mont.rand$ds.est, paired = T, alternative = 'greater')
```
This returned a p-values of `r test$p.value` indicating at any significance level, the null hypothesis of the diffeence between the estimates being zero is rejected in favour of the alternate, that the DSM estimates are greater than those of DS.

``` {r resMontrand}
results.rand <- read.csv('Results/results Montrave random.csv')

results.rand$X <- NULL

results.rand <-  results.rand %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.rand,
      caption = 'Buckland 2015 example results - Random Line') %>%
        add_header_above(c('Statistic' = 1, 'Random Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "HOLD_position")
```
In Table \@ref(tab:resMontrand) above, we can see the various statistics extracted during the course of the simulations. As we have seen throughout throughout this report, the mean for the DSM is a slight overestimates and the DS is a slight underestimates. This is transferred into the bias, with the DSM having a positve bias and DS having a negative bias, however both of these are below the 5% level so no concerns are raised for either method. Unfortunately, there are major concerns regarding the Mean Std errors of the estimates, which are the estimator of the Std deviations. For both methods, it can be seen that they are underestimates of the Std deviations and very severely in the case of the DSM method. This is problematic as not capturing all of the variance can lead to the the results assuming they are more accurate than they actually are. These underestimations of variance are translated into the confidence interval coverage, where both fail to contain truth at the 95% confidence level. Additionally, the skewness and kurtosis statistics are very different for the DSM method, likely as a result of the extreme overestimations produce by some survey realisation. Overall, it could be concluded that in this scenario, the DS method performed better, by virtue of underestimating the variance less and having less extreme estimates, however neither method should be considered adequate in this case.

\pagebreak
## Results - Systematic Parallel Line

Moving on the systematic parallel line design, histograms of the abundance estimates are plotted below in Figure \@ref(fig:histMontline).
```{r histMontline, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Systematic Parallel Line Design estimates for Buckland 2015 example region'}
Mont.para <-  read.csv('Estimates/Montrave5000lineparallel.csv')

hist(Mont.para$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(Mont.para$dsm.est), col = 'red')

hist(Mont.para$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(Mont.para$ds.est), col = 'red')
```

These plots in Figure \@ref(fig:histMontline) appear to be a significant improvement over those seen with the random line design. This can be determined based on the significant reduction in the range of estimates, especially in the case of the DSM approach. The mean of the estimates, also appears very close to truth at 1500, with some indication that the DSM approach has slightly overestimated. A more detailed analysis of the simulation results can be seen in Table \@ref(tab:resMontline).

``` {r resMontline}
results.para <- read.csv('Results/results Montrave parallel.csv')

results.para$X <- NULL

results.para <-  results.para %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.para,
      caption = 'Buckland 2015 example results - Parallel Line') %>%
        add_header_above(c('Statistic' = 1, 'Systematic Parallel Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

From the results in Table \@ref(tab:resMontline) the overestimation implied from the histograms is clearly seen, with the mean of the abundance estimates having a bias of almost 2.5%. However, when it comes to the variance estimation, the DSM approach is far superior in this case. While the true variance, the Std Deviation of the estimates, is lower for the DS approach, the estimate of this, the Mean Std error of the estimates is dramatically larger than what it is trying to estimate. In contrast the DSM variance estimate is very close to the true variance. These differences in variance estimation the propagate into the coefficients of variation, with again the DSM method having a lower CV. These also have an effect on the confidence interval coverage, with the DS results capturing truth over 99% of the time. However, this is most likely due to the high variance estimate for this approach widening the confidence intervals. Despite the DSM method having a lower confidence interval coverage, both methods achieved coverage at the 95% level resulting in no cause for concern with either method. The Skewness and Kurtosis statistics are near identical for both sets of abundance estimates and appear close to those of a normal distribution.  In the Jarque-Bera test for normality, DSM and DS estimates return p-values of `r jarque.test(Mont.para$dsm.est)$p.value` and `r jarque.test(Mont.para$ds.est)$p.value` respectively. These indicate that we can reject the null hypothesis of normality for both sets of estimates in favour of the alternate that they are not normally distributed.


## Results - Zigzag Line

The final simulation run as part of the case study in @Buckland2015 used a zigzag design, the results of which are displayed below:

```{r histMontzig, fig.show = 'hold', out.width='50%', fig.cap='Histograms of Zigzag Line Design estimates for Buckland 2015 example region'}
Mont.zig <-  read.csv('Estimates/Montrave5000linezigzag.csv')

hist(Mont.zig$dsm.est,breaks = 50,
     main = 'Histogram of DSM estimates',
     xlab = 'DSM Abundance estimates')
abline(v = mean(Mont.zig$dsm.est), col = 'red')

hist(Mont.zig$ds.est,breaks = 50, 
     main = 'Histogram of DS estimates',
     xlab = 'DS Abundance estimates')
abline(v = mean(Mont.zig$ds.est), col = 'red')
```

In Figure \@ref(fig:histMontzig) above the histograms of the abundance estimates for the DSM and DS approaches can be observed. These appear to follow a similar pattern to those seen in the systematic parallel line simulation above. Namely both histograms look fairly pointed, with the mean for the DSM estimates appearing to be a slight overestimate while the mean for the DS abundance estimates looks to be close to truth. The results are examined more in-depth in Table \@ref(tab:resMontzig) below:

``` {r resMontzig}
results.zig <- read.csv('Results/results Montrave zigzag.csv')

results.zig$X <- NULL

results.zig <-  results.zig %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.zig,
      caption = 'Buckland 2015 example results - Zigzag Line') %>%
        add_header_above(c('Statistic' = 1, 'Zigzag Line Design' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

The results seen within Table \@ref(tab:resMontzig) appear very similar to those seen for the systematic parallel line design in Table \@ref(tab:resMontline). Once again, the DSM approach has slightly overestimates the abundance, with a bias of over 2%, however this is below the level of 5% at which concerns could be raised. In contrast, the DS approach slightly underestimates the true abundance, having a bias of below 1%. The similarities continue into the variance estimates, where DSM does a far better job of estimating the true variance than the DS method. This leads to the DSM results having a lower coefficient of variation, a desirable attribute. However, when investigating the confidence interval coverage, the DS approach, as usual, perform better, with truth being within the confidence interval over 99% of the time, although, as has been noted previously several times, this is likely due to the large variance estimate. In contrast, the coverage from the DSM method is just below 95% raising some elements of concern about its ability to cover truth within the confidence interval. The skewness and kurtosis statistics agree with the initial observations of the histograms, with the kurtosis suggesting a slightly leptokurtic distribution, one with larger tails and more pointed in the centre than that of the normal distribution, while the skewness suggests a slight right skew to the estimates, indicating a bias towards higher estimates. Overall, it could be concluded that the DSM approach proved superior in this case, mainly due to the accuracy of the variance estimation over the DS approach. While the Bias and confidence interval coverage were not as good as those for the DS method, I would not consider them sufficiently concerning to out-way the improvement in variance estimation.

## Discussion of Results

Additionally, both the Systemic parallel line design and the zigzag design were selected and specified such that they had a comparable cost to the survey. This cost is the trackline length, which is to total distance required to travel along each transect and includes the distances between transects. This is often termed the cost of a study as the distance travelled during a study is a major factor in the overall cost of a study, such as through fuel costs for air or seaborne studies as well as the time and resources associated with additional distances travelled. In these designs, the mean trackline length was around 700km for both survey designs. This allows for the results of the two designs to be compared and would be one of the steps conducted when selecting an appropriate survey design in the lead up to a real work survey. Comparing the results in both Tables \@ref(tab:resMontline) and \@ref(tab:resMontzig), it can be seen that the mean estimates from the DSM method are consistent accross the two survey designs, with a similar level of positive bias present in both. With the DS results, there appears to be a reduction in bias when using the parallel line design. Looking at variance estimation, the variance estimates from the zigzag designs are lower for both methods, indicating an increase in the precision as a result of using the zigzag design. The coefficients of variation stayed similar across the different designs as did the confidence interval coverage, with the skewness and kurtosis statistics also changing little between the two designs. Overall these results are consistent with those seen in @Buckland2015 with it being noted that the zigzag design offered a higher level of precision than the Mean Std error suggests. This has been confirmed by both the original DS method as well as the DSM approach, with this approach in fact giving improved estimates for the variance. It is however noted within @Buckland2015 that while the trackline length was similar for both designs, in situations where travelling on effort, i.e on a transect, is far more costly than off effort, the advantage gained from the zigzag design are negated by the addition cost. Despite this, in general, zigzag designs offer a higher level of efficiency due to the substantial increase in time spent on effort in comparison to parallel line designs. 

\pagebreak

# Simulation - Variance estimation tests {#ervar}

As has been noted through the analysis of previous simulations, the variance estimation of the DS method has often performed poorly in comparison with that of the DSM approach. As noted within Section \@ref(dsims), the default 'R2' encounter rate variance estimator has been used throughout all of the simulations so far, as according to @Fewster this shows good performance for random line designs. However, many of the simulations run have used systematic line designs, for which it is noted that 'R2' may not perform well. In this case, @Fewster recommends using the 'O2' estimator. To investigate if changing the variance estimator improves the performance of the DS method, we return to the non stratified North Sea region in Section \@ref(NS). Here, the simulation will follow the same systematic line survey used previously, as seen in Figure \@ref(fig:NSsurvey) alongside the density surface seen in Figure \@ref(fig:NSden). The detection function was also kept constant, using a half normal function with a scale parameter of 5, as seen in Figure \@ref(fig:NSdet). The only difference between the two simulations is the specification of the encounter rate variance estimator, with one simulation using the 'R2' estimator and the other using the more appropriate 'O2' estimator as recommended by @Fewster.

## Results

``` {r resR2est}
results.R2 <- read.csv('Results/results NS R2 estimator 5000.csv')

results.R2$X <- NULL

results.R2 <-  results.R2 %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.R2,
      caption = 'North Sea Line transect results with "R2" encounter rate variance estimator') %>%
        add_header_above(c('Statistic' = 1, '"R2" estimator' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

From Table \@ref(tab:resR2est) the same pattern as has been seen throughout this report is evident. The variance estimate, the Mean std error, for the DS method is far larger than the true variance, the Std Dev of the estimates, by over 60%. In comparison, the variance estimate for the DSM approach is only a slight overestimate at only around 10% higher. These then transfer into the coefficients of variation with the DSM having a respectively lower CV than that of the DS method. The confidence interval coverage for the DS method is very impressive, however this is likely due to the large variance estimates widening the confidence interval appreciably to contain truth. While the coverage for the DSM method is slightly lower, it is still above the 95% confidence limit and does not raise concerns. Overall in this case the DSM method can be said to have performed better, mainly due to the substantially better variance estimate and resulting coefficient of variation improvements over the DS method. 

``` {r resO2est}
results.O2<- read.csv('Results/results NS O2 estimator 5000.csv')

results.O2$X <- NULL

results.O2 <-  results.O2 %>%
        `rownames<-`(c('Mean',
                 'Bias',
                 'Mean Std error of estimates',
                 'Std Dev of estimates',
                 'Mean CV',
                 'Confidence Interval Coverage',
                 'Estimate Skewness',
                 'Estimate Kurtosis')) %>%
        `colnames<-`(c('DSM results', 'DS results'))
kable(results.O2,
      caption = 'North Sea Line transect results with "O2" encounter rate variance estimator') %>%
        add_header_above(c('Statistic' = 1, '"O2" estimator' = 2)) %>%
        kable_styling(position = 'center',
                      latex_options = "hold_position")
```

Moving on now to look at the results in Table \@ref(tab:resO2est), from when the variance estimator is set as the 'O2' estimator. It can be seen that there has been a significant improvement in the variance estimation in comparison to the 'R2' results above. The variance estimates for both methods are near identical now, with both being an overestimate of the true variance by around 10%. It appears that while the variance estimate for the DSM method is slightly closer than the DS approach, the difference is very marginal, highlighting the improvements to the estimation in the DS method. The confidence interval coverage for DS is slightly lower than previously, with this being the only statistic which appears worse for the 'O2' estimator than the 'R2' estimator. However this is likely due to the more representative variance estimate shrinking the confidence interval. Despite this, both methods acheived a confidence interval coverage above the 95% level. Overall in this case there is very little to differentiate between the two methods. It could be considered that the DS method performed better, as given that all other statistics are near identical, the mean estimate is closer to truth and as a result the bias is slightly smaller. However, the DSM method still performed well, with its bias being below 1%. 

These simulations have highlighted the improvements which can be made to the DS method to improve its variance estimation and make it comparable or possibly better than the DS method, if the particular design allows.


\pagebreak
# Discussion



## Variance estimators within _dsims_.

As noted within Section \@ref(dsims), there are a variety of different encounter rate variance estimators available for distance sampling data, as detailed within @Fewster. However, at present, only a select few are available within the _dsims_ package and even these are limited based on the particular design chosen. For example, while the 'O2' estimator tested during Section \@ref(ervar) proved to markedly improve the variance estimation, this estimator is only available within the existing simulations when using systematic parallel line designs, and not with the very common zigzag design. While distance sampling analysis can be conducted with variance estimators not available within the simulation, this allows for a potentially unfair comparison between survey designs when the best available estimator is used, as opposed to the best overall and may results in an non-ideal design being selected. As identified throughout the majority of the simulations, the DS approach did not perform as well as the DSM method with regards to variance estimation, highlighting this is an area where substantial improves could be made.

## Limitations

While an number of different simulation have been conducted throughout this report, it is noted that the majority of these are fairly basic in nature and aim to show that the simulation method developed is able to sufficiently deal with issues encountered early on in the development process. These issues were namely the creation of the segments required for the DSM, with the Buckland 2015 example introducing the most errors as a result of the islands presents which had to be overcome. To that end, there are a number of other areas which I wished to investigate, had time allowed, which can be split into two separate and almost distinct areas, variance estimators within _dsims_ and further more in depth simulations to determine the limits of the model and simulations.

## Further simulations

At the current stage, none of the simulations have looked at covariates within either the detection function or the models themselves. As seen within the example study conducted within @Miller2013, an example of this is using depth within seaborne surveys as a covariate within the models. Due to the likelihood of covariates being considered within both distance sampling and density surface modelling approached, an interesting set of simulations would be to look at how the use of covariates influences the results generates by each method and indeed if the coefficients of these vary between the approaches. At the current moment, the DSM simulations developed are not capable of implementing this, however the additional work required would not be major and would involved ensuring any covariate information is appropriately passed to the relevant segments. The model could then be generalised to account for the additional information and allow these additional factors to be implemented.

Additionally, when discussing covariates and the segments, @Miller2013 suggests that the segment size for DSM should be chosen such that neither the density nor covariates should change substantially within the segment. A potentially interesting comparison could arise by running simulations in which this was not the case, where a covariate significantly varies within segments. This would establish how badly the DSM performs when the segments are not sufficiently small and if distance sampling offers a viable alternative in this case. 

Leading on from this, when generating predictions outside the original study area, the estimates may be very different based upon the density prescribed at the edge. @Miller2013 highlights that at edges of the study area, both internal and external, models may link these areas and align the density between the two, without taking account for the area for which no animals may occur. They term this as 'smoothed across', implying that predictions could in theory generate unphysical abundance estimates, i.e. dolphins present on an island. When constructing the simulations, care was taken to ensure that the prediction area was cropped to the study region in an attempt to remove this effect. However, it is noted that a soap film smoother, a function of spatial location, could be used to account for these complex areas. Also mentioned within @Miller2013 is that even with smoothers, the density profile at the edge can still cause issues. There is the potential for smoothers to have covariates which cause the models fitted density surface to behave unrealistically when predicting the surface a substantial distance from the survey effort. @Miller2013 does note that this issue may be reduces by selecting an alternate smoother, with a generalisation of Duchon Splines suggested. A further area of study could focus on investigating these effects further, with the potential aim of constructing smoothers which are less prone to exhibiting this behaviour.

\pagebreak

# Conclusion

A number of interesting conclusions can be drawn from the various simulations conducted throughout this report. One of the aims of this report was to establish that DSM had be successfully implemented into the simulation set-up alongside the existing distance sampling method. From the results of all of the simulations above, there is little evidence to suggest that this was not successful, however, as noted within the discussion, further more demanding simulations should be conducted to verify this further. Throughout the simulations conducted, the DSM approach has performed well in comparison to the existing distance sampling approach, in some case being better and in others worse. While there is some evidence that the abundance estimates for point transect designs may not be as close to truth as one may wish, the simulation in which DSM performed significantly worse was the random parallel line design from the Buckland 2015 example case study. This study showed that the DSM approach severely underestimated the true variance of the estimates, a major concern, as well as its confidence interval coverage being below 90%.

- for comparable designs, DSM proved more precise
- random designs lead to severe variance underestimation
- using better variance estimators makes DS more comparable to DSM
- dsm higher estimates

\pagebreak
# References {-}
